{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/theboredman/CSE468/blob/main/Quiz_1/CNN/Using_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN (CIFAR 10 Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BznMYM4BOwvq"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I5dp3f2tOwvq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # @param [\"tensorflow\", \"jax\", \"torch\"]\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import ops\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZKoW6mNOwvq"
   },
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QyJ7EmyOOwvr",
    "outputId": "88a2c848-efa7-4d81-84b2-df3b7c79645a"
   },
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D09NY6oNOwvr"
   },
   "source": [
    "## Configure the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uUj-whd4Owvr"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 256  # Reduced for better gradient updates\n",
    "num_epochs = 10  # Increased for better training\n",
    "image_size = 72  # We'll resize input images to this size\n",
    "mlp_head_units = [\n",
    "    2048,\n",
    "    1024,\n",
    "]  # Size of the dense layers of the final classifier\n",
    "\n",
    "# New hyperparameters for improved training\n",
    "initial_learning_rate = 0.001\n",
    "label_smoothing = 0.1\n",
    "dropout_rate = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCG6llz9Owvr"
   },
   "source": [
    "## Use data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EqkQlgZYOwvs"
   },
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(image_size, image_size),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.1),  # Increased rotation\n",
    "        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
    "        layers.RandomTranslation(height_factor=0.1, width_factor=0.1),  # Added translation\n",
    "        layers.RandomContrast(factor=0.2),  # Added contrast adjustment\n",
    "        layers.RandomBrightness(factor=0.2),  # Added brightness adjustment\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCbpyizwOwvs"
   },
   "source": [
    "## Implement multilayer perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t9vc7ZScOwvs"
   },
   "outputs": [],
   "source": [
    "\n",
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=keras.activations.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qY1MRkfDOwvs"
   },
   "source": [
    "## CNN Architecture\n",
    "\n",
    "We'll build a CNN with convolutional layers for feature extraction followed by dense layers for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r0eACAmmOwvt"
   },
   "outputs": [],
   "source": [
    "def residual_block(x, filters, kernel_size=3, stride=1):\n",
    "    \"\"\"Creates a residual block with skip connection\"\"\"\n",
    "    shortcut = x\n",
    "\n",
    "    # First conv layer\n",
    "    x = layers.Conv2D(filters, kernel_size, strides=stride, padding='same',\n",
    "                     kernel_initializer='he_normal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # Second conv layer\n",
    "    x = layers.Conv2D(filters, kernel_size, strides=1, padding='same',\n",
    "                     kernel_initializer='he_normal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # Adjust shortcut if needed\n",
    "    if stride != 1 or shortcut.shape[-1] != filters:\n",
    "        shortcut = layers.Conv2D(filters, 1, strides=stride, padding='same',\n",
    "                               kernel_initializer='he_normal')(shortcut)\n",
    "        shortcut = layers.BatchNormalization()(shortcut)\n",
    "\n",
    "    # Add shortcut and apply activation\n",
    "    x = layers.Add()([x, shortcut])\n",
    "    x = layers.ReLU()(x)\n",
    "    return x\n",
    "\n",
    "def separable_conv_block(x, filters, kernel_size=3, stride=1):\n",
    "    \"\"\"Creates a depthwise separable convolution block\"\"\"\n",
    "    x = layers.SeparableConv2D(filters, kernel_size, strides=stride, padding='same',\n",
    "                              depthwise_initializer='he_normal',\n",
    "                              pointwise_initializer='he_normal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    return x\n",
    "\n",
    "def create_cnn_classifier():\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    # Augment data.\n",
    "    x = data_augmentation(inputs)\n",
    "\n",
    "    # Initial convolution\n",
    "    x = layers.Conv2D(32, (3, 3), strides=1, padding='same',\n",
    "                     kernel_initializer='he_normal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # First residual block\n",
    "    x = residual_block(x, 64, stride=1)\n",
    "    x = residual_block(x, 64, stride=1)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "\n",
    "    # Second residual block\n",
    "    x = residual_block(x, 128, stride=1)\n",
    "    x = residual_block(x, 128, stride=1)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "\n",
    "    # Third block with separable convolutions\n",
    "    x = separable_conv_block(x, 256, stride=1)\n",
    "    x = separable_conv_block(x, 256, stride=1)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    # Fourth block\n",
    "    x = residual_block(x, 512, stride=1)\n",
    "    x = layers.GlobalAveragePooling2D()(x)  # Better than Flatten for spatial invariance\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "\n",
    "    # Dense layers with residual connection\n",
    "    dense_input = x\n",
    "    x = layers.Dense(512, kernel_initializer='he_normal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = layers.Dense(256, kernel_initializer='he_normal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    # Add MLP classification head\n",
    "    features = mlp(x, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "\n",
    "    # Final classification layer\n",
    "    logits = layers.Dense(num_classes, kernel_initializer='he_normal')(features)\n",
    "\n",
    "    # Create the Keras model\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5AfzEtOOwvt"
   },
   "source": [
    "## Compile, train, and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "jeN5G9j-Owvt",
    "outputId": "f92c09c1-fcd3-4b4a-9756-e245027b1a1e"
   },
   "outputs": [],
   "source": [
    "def create_callbacks():\n",
    "    \"\"\"Create advanced callbacks for better training\"\"\"\n",
    "    callbacks = [\n",
    "        # Model checkpoint\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            \"/tmp/best_model.weights.h5\",\n",
    "            monitor=\"val_accuracy\",\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "\n",
    "        # Early stopping\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_accuracy\",\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "\n",
    "        # Reduce learning rate on plateau\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor=\"val_accuracy\",\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "\n",
    "        # Cosine annealing learning rate schedule\n",
    "        keras.callbacks.LearningRateScheduler(\n",
    "            lambda epoch: initial_learning_rate * 0.5 * (1 + np.cos(np.pi * epoch / num_epochs)),\n",
    "            verbose=0\n",
    "        )\n",
    "    ]\n",
    "    return callbacks\n",
    "\n",
    "def run_experiment(model):\n",
    "    # Use AdamW with better parameters\n",
    "    optimizer = keras.optimizers.AdamW(\n",
    "        learning_rate=initial_learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999\n",
    "    )\n",
    "\n",
    "    # Apply label smoothing to y_train\n",
    "    y_train_smoothed = keras.utils.to_categorical(y_train, num_classes=num_classes)\n",
    "    y_train_smoothed = y_train_smoothed * (1 - label_smoothing) + label_smoothing / num_classes\n",
    "\n",
    "\n",
    "    # Compile with label smoothing\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.CategoricalCrossentropy(\n",
    "            from_logits=True\n",
    "        ),\n",
    "        metrics=[\n",
    "            keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Get callbacks\n",
    "    callbacks = create_callbacks()\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train_smoothed,  # Use smoothed labels here\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_split=0.15,  # Increased validation split\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Load best weights\n",
    "    model.load_weights(\"/tmp/best_model.weights.h5\")\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_loss, accuracy, top_5_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(f\"\\nFinal Test Results:\")\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "    print(f\"Test loss: {round(test_loss, 4)}\")\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "# Create and train the enhanced model\n",
    "cnn_classifier = create_cnn_classifier()\n",
    "\n",
    "# Print model summary\n",
    "print(\"Model Architecture:\")\n",
    "cnn_classifier.summary()\n",
    "\n",
    "# Train the model\n",
    "history = run_experiment(cnn_classifier)\n",
    "\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Plot comprehensive training history\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    # Plot loss\n",
    "    axes[0, 0].plot(history.history['loss'], label='Training Loss')\n",
    "    axes[0, 0].plot(history.history['val_loss'], label='Validation Loss')\n",
    "    axes[0, 0].set_title('Model Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "\n",
    "    # Plot accuracy\n",
    "    axes[0, 1].plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    axes[0, 1].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    axes[0, 1].set_title('Model Accuracy')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "\n",
    "    # Plot top-5 accuracy\n",
    "    axes[1, 0].plot(history.history['top-5-accuracy'], label='Training Top-5 Accuracy')\n",
    "    axes[1, 0].plot(history.history['val_top-5-accuracy'], label='Validation Top-5 Accuracy')\n",
    "    axes[1, 0].set_title('Model Top-5 Accuracy')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Top-5 Accuracy')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True)\n",
    "\n",
    "    # Plot learning rate\n",
    "    if 'lr' in history.history:\n",
    "        axes[1, 1].plot(history.history['lr'])\n",
    "        axes[1, 1].set_title('Learning Rate Schedule')\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Learning Rate')\n",
    "        axes[1, 1].set_yscale('log')\n",
    "        axes[1, 1].grid(True)\n",
    "    else:\n",
    "        axes[1, 1].text(0.5, 0.5, 'Learning rate data not available',\n",
    "                       ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "        axes[1, 1].set_title('Learning Rate Schedule')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the training history\n",
    "plot_training_history(history)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "image_classification_with_vision_transformer",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
