{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f92af4be",
   "metadata": {
    "id": "f92af4be"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/theboredman/CSE468/blob/main/Quiz_1/CNN/Using_CNN_CIFAR100_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8856f835",
   "metadata": {
    "id": "8856f835"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48037d5c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10310,
     "status": "ok",
     "timestamp": 1760816394452,
     "user": {
      "displayName": "Asadullah Hil Galib",
      "userId": "02825503865158832131"
     },
     "user_tz": -360
    },
    "id": "48037d5c",
    "outputId": "f2b040b0-855a-496a-a2cc-e5760adeb46d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR100\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550841e0",
   "metadata": {
    "id": "550841e0"
   },
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a38ffc11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7444,
     "status": "ok",
     "timestamp": 1760816401893,
     "user": {
      "displayName": "Asadullah Hil Galib",
      "userId": "02825503865158832131"
     },
     "user_tz": -360
    },
    "id": "a38ffc11",
    "outputId": "bcb1e207-2d56-41e7-e1bb-11b7b4d01843"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169M/169M [00:03<00:00, 43.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 50000\n",
      "Test samples: 10000\n"
     ]
    }
   ],
   "source": [
    "num_classes = 100\n",
    "input_shape = (3, 32, 32)  # PyTorch uses (C, H, W) format\n",
    "\n",
    "# Define transforms for training and testing\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((72, 72)),  # Resize to match the original code\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=18),  # 0.1 * 180 degrees\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Random translation\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((72, 72)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
    "])\n",
    "\n",
    "# Load CIFAR-100 dataset\n",
    "train_dataset = CIFAR100(root='./data', train=True, download=True, transform=train_transform)\n",
    "test_dataset = CIFAR100(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e76953",
   "metadata": {
    "id": "98e76953"
   },
   "source": [
    "## Configure the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ef88639",
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1760816401913,
     "user": {
      "displayName": "Asadullah Hil Galib",
      "userId": "02825503865158832131"
     },
     "user_tz": -360
    },
    "id": "8ef88639"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 128  # Reduced for better gradient updates\n",
    "num_epochs = 10  # Increased for better training\n",
    "image_size = 72  # We'll resize input images to this size\n",
    "mlp_head_units = [\n",
    "    2048,\n",
    "    1024,\n",
    "]  # Size of the dense layers of the final classifier\n",
    "\n",
    "# New hyperparameters for improved training\n",
    "initial_learning_rate = 0.001\n",
    "label_smoothing = 0.1\n",
    "dropout_rate = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fd44c9",
   "metadata": {
    "id": "63fd44c9"
   },
   "source": [
    "## Implement multilayer perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89fa985a",
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1760816401926,
     "user": {
      "displayName": "Asadullah Hil Galib",
      "userId": "02825503865158832131"
     },
     "user_tz": -360
    },
    "id": "89fa985a"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_units, dropout_rate):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "\n",
    "        for units in hidden_units:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, units),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_dim = units\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ad32cd",
   "metadata": {
    "id": "74ad32cd"
   },
   "source": [
    "## CNN Architecture\n",
    "\n",
    "We'll build a CNN with convolutional layers for feature extraction followed by dense layers for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c47539a3",
   "metadata": {
    "executionInfo": {
     "elapsed": 165,
     "status": "ok",
     "timestamp": 1760816402101,
     "user": {
      "displayName": "Asadullah Hil Galib",
      "userId": "02825503865158832131"
     },
     "user_tz": -360
    },
    "id": "c47539a3"
   },
   "outputs": [],
   "source": [
    "class SqueezeExcitationBlock(nn.Module):\n",
    "    \"\"\"Squeeze-and-Excitation block for attention mechanism\"\"\"\n",
    "    def __init__(self, channels, ratio=16):\n",
    "        super(SqueezeExcitationBlock, self).__init__()\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Linear(channels, channels // ratio)\n",
    "        self.fc2 = nn.Linear(channels // ratio, channels)\n",
    "        self.swish = nn.SiLU()  # Swish activation\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        se = self.global_pool(x).view(b, c)\n",
    "        se = self.swish(self.fc1(se))\n",
    "        se = self.sigmoid(self.fc2(se)).view(b, c, 1, 1)\n",
    "        return x * se\n",
    "\n",
    "class ImprovedResidualBlock(nn.Module):\n",
    "    \"\"\"Enhanced residual block with Group Normalization and Swish activation\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, use_se=True):\n",
    "        super(ImprovedResidualBlock, self).__init__()\n",
    "        self.use_se = use_se\n",
    "\n",
    "        # First conv layer with Group Normalization\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=kernel_size//2, bias=False)\n",
    "        self.gn1 = nn.GroupNorm(8, out_channels)\n",
    "\n",
    "        # Second conv layer\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, 1, padding=kernel_size//2, bias=False)\n",
    "        self.gn2 = nn.GroupNorm(8, out_channels)\n",
    "\n",
    "        # Squeeze-and-Excitation\n",
    "        if use_se:\n",
    "            self.se = SqueezeExcitationBlock(out_channels)\n",
    "\n",
    "        # Shortcut connection\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1, stride, bias=False),\n",
    "                nn.GroupNorm(8, out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "        self.swish = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.shortcut(x)\n",
    "\n",
    "        out = self.swish(self.gn1(self.conv1(x)))\n",
    "        out = self.gn2(self.conv2(out))\n",
    "\n",
    "        if self.use_se:\n",
    "            out = self.se(out)\n",
    "\n",
    "        out = out + residual\n",
    "        out = self.swish(out)\n",
    "        return out\n",
    "\n",
    "class EfficientConvBlock(nn.Module):\n",
    "    \"\"\"EfficientNet-style inverted residual block\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, expansion_factor=4):\n",
    "        super(EfficientConvBlock, self).__init__()\n",
    "        expanded_channels = out_channels * expansion_factor\n",
    "\n",
    "        # Expansion phase\n",
    "        if expansion_factor != 1:\n",
    "            self.expand = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, expanded_channels, 1, bias=False),\n",
    "                nn.GroupNorm(8, expanded_channels),\n",
    "                nn.SiLU()\n",
    "            )\n",
    "            in_channels = expanded_channels\n",
    "        else:\n",
    "            self.expand = nn.Identity()\n",
    "\n",
    "        # Depthwise convolution\n",
    "        self.depthwise = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding=kernel_size//2,\n",
    "                     groups=in_channels, bias=False),\n",
    "            nn.GroupNorm(8, in_channels),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "        # Squeeze-and-Excitation\n",
    "        self.se = SqueezeExcitationBlock(in_channels)\n",
    "\n",
    "        # Projection phase\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.GroupNorm(8, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.expand(x)\n",
    "        x = self.depthwise(x)\n",
    "        x = self.se(x)\n",
    "        x = self.project(x)\n",
    "        return x\n",
    "\n",
    "class CBAMAttentionBlock(nn.Module):\n",
    "    \"\"\"Convolutional Block Attention Module (CBAM)\"\"\"\n",
    "    def __init__(self, channels, ratio=16):\n",
    "        super(CBAMAttentionBlock, self).__init__()\n",
    "        # Channel Attention\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc1 = nn.Linear(channels, channels // ratio)\n",
    "        self.fc2 = nn.Linear(channels // ratio, channels)\n",
    "        self.swish = nn.SiLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # Spatial Attention\n",
    "        self.spatial_conv = nn.Conv2d(2, 1, 7, padding=3, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.size()\n",
    "\n",
    "        # Channel Attention\n",
    "        avg_pool = self.avg_pool(x).view(b, c)\n",
    "        max_pool = self.max_pool(x).view(b, c)\n",
    "\n",
    "        avg_out = self.sigmoid(self.fc2(self.swish(self.fc1(avg_pool))))\n",
    "        max_out = self.sigmoid(self.fc2(self.swish(self.fc1(max_pool))))\n",
    "\n",
    "        channel_attention = (avg_out + max_out).view(b, c, 1, 1)\n",
    "        x = x * channel_attention\n",
    "\n",
    "        # Spatial Attention\n",
    "        avg_pool_spatial = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_pool_spatial, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        spatial_concat = torch.cat([avg_pool_spatial, max_pool_spatial], dim=1)\n",
    "\n",
    "        spatial_attention = self.sigmoid(self.spatial_conv(spatial_concat))\n",
    "        x = x * spatial_attention\n",
    "\n",
    "        return x\n",
    "\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=100, mlp_head_units=[2048, 1024]):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "\n",
    "        # Stem: Initial feature extraction\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, 48, 3, padding=1, bias=False),\n",
    "            nn.GroupNorm(8, 48),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "        # Initial attention\n",
    "        self.initial_attention = CBAMAttentionBlock(48)\n",
    "\n",
    "        # Stage 1: Enhanced residual blocks with attention\n",
    "        self.stage1 = nn.Sequential(\n",
    "            ImprovedResidualBlock(48, 64, stride=1, use_se=True),\n",
    "            ImprovedResidualBlock(64, 64, stride=1, use_se=True),\n",
    "            ImprovedResidualBlock(64, 64, stride=2, use_se=True),\n",
    "            nn.Dropout2d(0.15)\n",
    "        )\n",
    "\n",
    "        # Stage 2: EfficientNet-style blocks\n",
    "        self.stage2 = nn.Sequential(\n",
    "            EfficientConvBlock(64, 96, stride=1, expansion_factor=4),\n",
    "            EfficientConvBlock(96, 96, stride=1, expansion_factor=4),\n",
    "            CBAMAttentionBlock(96),\n",
    "            EfficientConvBlock(96, 96, stride=2, expansion_factor=4),\n",
    "            nn.Dropout2d(0.2)\n",
    "        )\n",
    "\n",
    "        # Stage 3: Mixed convolution types\n",
    "        self.stage3 = nn.Sequential(\n",
    "            ImprovedResidualBlock(96, 144, stride=1, use_se=True),\n",
    "            EfficientConvBlock(144, 144, stride=1, expansion_factor=6),\n",
    "            ImprovedResidualBlock(144, 144, stride=1, use_se=True),\n",
    "            CBAMAttentionBlock(144),\n",
    "            nn.AvgPool2d(2),\n",
    "            nn.Dropout2d(0.25)\n",
    "        )\n",
    "\n",
    "        # Stage 4: High-level feature extraction\n",
    "        self.stage4 = nn.Sequential(\n",
    "            EfficientConvBlock(144, 192, stride=1, expansion_factor=6),\n",
    "            ImprovedResidualBlock(192, 192, stride=1, use_se=True),\n",
    "            EfficientConvBlock(192, 192, stride=1, expansion_factor=6),\n",
    "            CBAMAttentionBlock(192),\n",
    "            nn.Dropout2d(0.3)\n",
    "        )\n",
    "\n",
    "        # Stage 5: Final feature maps\n",
    "        self.stage5 = nn.Sequential(\n",
    "            ImprovedResidualBlock(192, 256, stride=1, use_se=True),\n",
    "            EfficientConvBlock(256, 256, stride=1, expansion_factor=8)\n",
    "        )\n",
    "\n",
    "        # Multi-scale feature aggregation\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.global_max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        # Enhanced classifier head with Ghost modules\n",
    "        self.classifier_head = nn.Sequential(\n",
    "            nn.Linear(512, 768, bias=False),  # 256*2 from concatenated pooling\n",
    "            nn.GroupNorm(1, 768),  # LayerNorm equivalent\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            # Ghost bottleneck\n",
    "            nn.Linear(768, 384, bias=False),\n",
    "            nn.GroupNorm(1, 384),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.4)\n",
    "        )\n",
    "\n",
    "        # MLP classification head\n",
    "        self.mlp = MLP(384, mlp_head_units, 0.5)\n",
    "\n",
    "        # Final classification layer\n",
    "        self.final_classifier = nn.Linear(mlp_head_units[-1], num_classes)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, (nn.GroupNorm, nn.BatchNorm2d)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Stem\n",
    "        x = self.stem(x)\n",
    "        x = self.initial_attention(x)\n",
    "\n",
    "        # Stages\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "        x = self.stage5(x)\n",
    "\n",
    "        # Multi-scale pooling\n",
    "        gap = self.global_avg_pool(x).flatten(1)\n",
    "        gmp = self.global_max_pool(x).flatten(1)\n",
    "        x = torch.cat([gap, gmp], dim=1)\n",
    "\n",
    "        # Classifier\n",
    "        x = self.classifier_head(x)\n",
    "        x = self.mlp(x)\n",
    "        x = self.final_classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fa4748",
   "metadata": {
    "id": "10fa4748"
   },
   "source": [
    "## Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d1fb4b4",
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1760816402141,
     "user": {
      "displayName": "Asadullah Hil Galib",
      "userId": "02825503865158832131"
     },
     "user_tz": -360
    },
    "id": "6d1fb4b4"
   },
   "outputs": [],
   "source": [
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    \"\"\"Label smoothing cross entropy loss\"\"\"\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        log_prob = F.log_softmax(pred, dim=-1)\n",
    "        weight = pred.new_ones(pred.size()) * self.smoothing / (pred.size(-1) - 1.)\n",
    "        weight.scatter_(-1, target.unsqueeze(-1), (1. - self.smoothing))\n",
    "        loss = (-weight * log_prob).sum(dim=-1).mean()\n",
    "        return loss\n",
    "\n",
    "def calculate_accuracy(outputs, targets, topk=(1, 5, 10)):\n",
    "    \"\"\"Calculate top-k accuracy\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = targets.size(0)\n",
    "\n",
    "        _, pred = outputs.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(targets.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc='Training')\n",
    "    for batch_idx, (data, target) in enumerate(pbar):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = output.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            'Loss': f'{running_loss/(batch_idx+1):.3f}',\n",
    "            'Acc': f'{100.*correct/total:.2f}%'\n",
    "        })\n",
    "\n",
    "    return running_loss / len(train_loader), 100. * correct / total\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    top1_correct = 0\n",
    "    top5_correct = 0\n",
    "    top10_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader, desc='Validation')\n",
    "        for data, target in pbar:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            val_loss += criterion(output, target).item()\n",
    "\n",
    "            # Calculate top-k accuracies\n",
    "            acc1, acc5, acc10 = calculate_accuracy(output, target, topk=(1, 5, 10))\n",
    "            top1_correct += acc1.item() * target.size(0) / 100\n",
    "            top5_correct += acc5.item() * target.size(0) / 100\n",
    "            top10_correct += acc10.item() * target.size(0) / 100\n",
    "            total += target.size(0)\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                'Loss': f'{val_loss/(len(pbar.dataset)):.3f}',\n",
    "                'Top1': f'{100.*top1_correct/total:.2f}%',\n",
    "                'Top5': f'{100.*top5_correct/total:.2f}%'\n",
    "            })\n",
    "\n",
    "    return (val_loss / len(val_loader),\n",
    "            100. * top1_correct / total,\n",
    "            100. * top5_correct / total,\n",
    "            100. * top10_correct / total)\n",
    "\n",
    "def cosine_annealing_lr(epoch, total_epochs, initial_lr):\n",
    "    \"\"\"Cosine annealing learning rate schedule\"\"\"\n",
    "    return initial_lr * 0.5 * (1 + np.cos(np.pi * epoch / total_epochs))\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs, device):\n",
    "    # Loss function with label smoothing\n",
    "    criterion = LabelSmoothingCrossEntropy(smoothing=label_smoothing)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = optim.AdamW(model.parameters(),\n",
    "                           lr=initial_learning_rate,\n",
    "                           weight_decay=weight_decay,\n",
    "                           betas=(0.9, 0.999),\n",
    "                           eps=1e-7)\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.LambdaLR(optimizer,\n",
    "                                           lambda epoch: cosine_annealing_lr(epoch, num_epochs, 1.0))\n",
    "\n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'val_loss': [], 'val_acc': [], 'val_top5_acc': [], 'val_top10_acc': [],\n",
    "        'lr': []\n",
    "    }\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    patience = 10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 50)\n",
    "\n",
    "        # Training\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "\n",
    "        # Validation\n",
    "        val_loss, val_acc, val_top5_acc, val_top10_acc = validate(model, val_loader, criterion, device)\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_top5_acc'].append(val_top5_acc)\n",
    "        history['val_top10_acc'].append(val_top10_acc)\n",
    "        history['lr'].append(current_lr)\n",
    "\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, Val Top5: {val_top5_acc:.2f}%, Val Top10: {val_top10_acc:.2f}%')\n",
    "        print(f'Learning Rate: {current_lr:.6f}')\n",
    "\n",
    "        # Early stopping and model checkpoint\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f'New best validation accuracy: {best_val_acc:.2f}%')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch+1}')\n",
    "            break\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3647fa",
   "metadata": {
    "id": "3e3647fa"
   },
   "source": [
    "## Compile, train, and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6843a450",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 143895,
     "status": "error",
     "timestamp": 1760816546038,
     "user": {
      "displayName": "Asadullah Hil Galib",
      "userId": "02825503865158832131"
     },
     "user_tz": -360
    },
    "id": "6843a450",
    "outputId": "7f8c8b90-9b8e-418d-ccb4-ac14e62e6cee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Architecture:\n",
      "CNNClassifier(\n",
      "  (stem): Sequential(\n",
      "    (0): Conv2d(3, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): GroupNorm(8, 48, eps=1e-05, affine=True)\n",
      "    (2): SiLU()\n",
      "  )\n",
      "  (initial_attention): CBAMAttentionBlock(\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "    (fc1): Linear(in_features=48, out_features=3, bias=True)\n",
      "    (fc2): Linear(in_features=3, out_features=48, bias=True)\n",
      "    (swish): SiLU()\n",
      "    (sigmoid): Sigmoid()\n",
      "    (spatial_conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "  )\n",
      "  (stage1): Sequential(\n",
      "    (0): ImprovedResidualBlock(\n",
      "      (conv1): Conv2d(48, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (gn1): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (gn2): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "      (se): SqueezeExcitationBlock(\n",
      "        (global_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc1): Linear(in_features=64, out_features=4, bias=True)\n",
      "        (fc2): Linear(in_features=4, out_features=64, bias=True)\n",
      "        (swish): SiLU()\n",
      "        (sigmoid): Sigmoid()\n",
      "      )\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(48, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (swish): SiLU()\n",
      "    )\n",
      "    (1): ImprovedResidualBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (gn1): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (gn2): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "      (se): SqueezeExcitationBlock(\n",
      "        (global_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc1): Linear(in_features=64, out_features=4, bias=True)\n",
      "        (fc2): Linear(in_features=4, out_features=64, bias=True)\n",
      "        (swish): SiLU()\n",
      "        (sigmoid): Sigmoid()\n",
      "      )\n",
      "      (shortcut): Identity()\n",
      "      (swish): SiLU()\n",
      "    )\n",
      "    (2): ImprovedResidualBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (gn1): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (gn2): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "      (se): SqueezeExcitationBlock(\n",
      "        (global_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc1): Linear(in_features=64, out_features=4, bias=True)\n",
      "        (fc2): Linear(in_features=4, out_features=64, bias=True)\n",
      "        (swish): SiLU()\n",
      "        (sigmoid): Sigmoid()\n",
      "      )\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (swish): SiLU()\n",
      "    )\n",
      "    (3): Dropout2d(p=0.15, inplace=False)\n",
      "  )\n",
      "  (stage2): Sequential(\n",
      "    (0): EfficientConvBlock(\n",
      "      (expand): Sequential(\n",
      "        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): GroupNorm(8, 384, eps=1e-05, affine=True)\n",
      "        (2): SiLU()\n",
      "      )\n",
      "      (depthwise): Sequential(\n",
      "        (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "        (1): GroupNorm(8, 384, eps=1e-05, affine=True)\n",
      "        (2): SiLU()\n",
      "      )\n",
      "      (se): SqueezeExcitationBlock(\n",
      "        (global_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc1): Linear(in_features=384, out_features=24, bias=True)\n",
      "        (fc2): Linear(in_features=24, out_features=384, bias=True)\n",
      "        (swish): SiLU()\n",
      "        (sigmoid): Sigmoid()\n",
      "      )\n",
      "      (project): Sequential(\n",
      "        (0): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (1): EfficientConvBlock(\n",
      "      (expand): Sequential(\n",
      "        (0): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): GroupNorm(8, 384, eps=1e-05, affine=True)\n",
      "        (2): SiLU()\n",
      "      )\n",
      "      (depthwise): Sequential(\n",
      "        (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "        (1): GroupNorm(8, 384, eps=1e-05, affine=True)\n",
      "        (2): SiLU()\n",
      "      )\n",
      "      (se): SqueezeExcitationBlock(\n",
      "        (global_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc1): Linear(in_features=384, out_features=24, bias=True)\n",
      "        (fc2): Linear(in_features=24, out_features=384, bias=True)\n",
      "        (swish): SiLU()\n",
      "        (sigmoid): Sigmoid()\n",
      "      )\n",
      "      (project): Sequential(\n",
      "        (0): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (2): CBAMAttentionBlock(\n",
      "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "      (fc1): Linear(in_features=96, out_features=6, bias=True)\n",
      "      (fc2): Linear(in_features=6, out_features=96, bias=True)\n",
      "      (swish): SiLU()\n",
      "      (sigmoid): Sigmoid()\n",
      "      (spatial_conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "    )\n",
      "    (3): EfficientConvBlock(\n",
      "      (expand): Sequential(\n",
      "        (0): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): GroupNorm(8, 384, eps=1e-05, affine=True)\n",
      "        (2): SiLU()\n",
      "      )\n",
      "      (depthwise): Sequential(\n",
      "        (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "        (1): GroupNorm(8, 384, eps=1e-05, affine=True)\n",
      "        (2): SiLU()\n",
      "      )\n",
      "      (se): SqueezeExcitationBlock(\n",
      "        (global_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc1): Linear(in_features=384, out_features=24, bias=True)\n",
      "        (fc2): Linear(in_features=24, out_features=384, bias=True)\n",
      "        (swish): SiLU()\n",
      "        (sigmoid): Sigmoid()\n",
      "      )\n",
      "      (project): Sequential(\n",
      "        (0): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (4): Dropout2d(p=0.2, inplace=False)\n",
      "  )\n",
      "  (stage3): Sequential(\n",
      "    (0): ImprovedResidualBlock(\n",
      "      (conv1): Conv2d(96, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (gn1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
      "      (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (gn2): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
      "      (se): SqueezeExcitationBlock(\n",
      "        (global_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc1): Linear(in_features=144, out_features=9, bias=True)\n",
      "        (fc2): Linear(in_features=9, out_features=144, bias=True)\n",
      "        (swish): SiLU()\n",
      "        (sigmoid): Sigmoid()\n",
      "      )\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(96, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (swish): SiLU()\n",
      "    )\n",
      "    (1): EfficientConvBlock(\n",
      "      (expand): Sequential(\n",
      "        (0): Conv2d(144, 864, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): GroupNorm(8, 864, eps=1e-05, affine=True)\n",
      "        (2): SiLU()\n",
      "      )\n",
      "      (depthwise): Sequential(\n",
      "        (0): Conv2d(864, 864, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=864, bias=False)\n",
      "        (1): GroupNorm(8, 864, eps=1e-05, affine=True)\n",
      "        (2): SiLU()\n",
      "      )\n",
      "      (se): SqueezeExcitationBlock(\n",
      "        (global_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc1): Linear(in_features=864, out_features=54, bias=True)\n",
      "        (fc2): Linear(in_features=54, out_features=864, bias=True)\n",
      "        (swish): SiLU()\n",
      "        (sigmoid): Sigmoid()\n",
      "      )\n",
      "      (project): Sequential(\n",
      "        (0): Conv2d(864, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (2): ImprovedResidualBlock(\n",
      "      (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (gn1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
      "      (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (gn2): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
      "      (se): SqueezeExcitationBlock(\n",
      "        (global_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc1): Linear(in_features=144, out_features=9, bias=True)\n",
      "        (fc2): Linear(in_features=9, out_features=144, bias=True)\n",
      "        (swish): SiLU()\n",
      "        (sigmoid): Sigmoid()\n",
      "      )\n",
      "      (shortcut): Identity()\n",
      "      (swish): SiLU()\n",
      "    )\n",
      "    (3): CBAMAttentionBlock(\n",
      "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "      (fc1): Linear(in_features=144, out_features=9, bias=True)\n",
      "      (fc2): Linear(in_features=9, out_features=144, bias=True)\n",
      "      (swish): SiLU()\n",
      "      (sigmoid): Sigmoid()\n",
      "      (spatial_conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "    )\n",
      "    (4): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (5): Dropout2d(p=0.25, inplace=False)\n",
      "  )\n",
      "  (stage4): Sequential(\n",
      "    (0): EfficientConvBlock(\n",
      "      (expand): Sequential(\n",
      "        (0): Conv2d(144, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
      "        (2): SiLU()\n",
      "      )\n",
      "      (depthwise): Sequential(\n",
      "        (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
      "        (1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
      "        (2): SiLU()\n",
      "      )\n",
      "      (se): SqueezeExcitationBlock(\n",
      "        (global_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc1): Linear(in_features=1152, out_features=72, bias=True)\n",
      "        (fc2): Linear(in_features=72, out_features=1152, bias=True)\n",
      "        (swish): SiLU()\n",
      "        (sigmoid): Sigmoid()\n",
      "      )\n",
      "      (project): Sequential(\n",
      "        (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ImprovedResidualBlock(\n",
      "      (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (gn1): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
      "      (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
      "      (se): SqueezeExcitationBlock(\n",
      "        (global_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc1): Linear(in_features=192, out_features=12, bias=True)\n",
      "        (fc2): Linear(in_features=12, out_features=192, bias=True)\n",
      "        (swish): SiLU()\n",
      "        (sigmoid): Sigmoid()\n",
      "      )\n",
      "      (shortcut): Identity()\n",
      "      (swish): SiLU()\n",
      "    )\n",
      "    (2): EfficientConvBlock(\n",
      "      (expand): Sequential(\n",
      "        (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
      "        (2): SiLU()\n",
      "      )\n",
      "      (depthwise): Sequential(\n",
      "        (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
      "        (1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
      "        (2): SiLU()\n",
      "      )\n",
      "      (se): SqueezeExcitationBlock(\n",
      "        (global_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc1): Linear(in_features=1152, out_features=72, bias=True)\n",
      "        (fc2): Linear(in_features=72, out_features=1152, bias=True)\n",
      "        (swish): SiLU()\n",
      "        (sigmoid): Sigmoid()\n",
      "      )\n",
      "      (project): Sequential(\n",
      "        (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (3): CBAMAttentionBlock(\n",
      "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "      (fc1): Linear(in_features=192, out_features=12, bias=True)\n",
      "      (fc2): Linear(in_features=12, out_features=192, bias=True)\n",
      "      (swish): SiLU()\n",
      "      (sigmoid): Sigmoid()\n",
      "      (spatial_conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "    )\n",
      "    (4): Dropout2d(p=0.3, inplace=False)\n",
      "  )\n",
      "  (stage5): Sequential(\n",
      "    (0): ImprovedResidualBlock(\n",
      "      (conv1): Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (gn1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (gn2): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "      (se): SqueezeExcitationBlock(\n",
      "        (global_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc1): Linear(in_features=256, out_features=16, bias=True)\n",
      "        (fc2): Linear(in_features=16, out_features=256, bias=True)\n",
      "        (swish): SiLU()\n",
      "        (sigmoid): Sigmoid()\n",
      "      )\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (swish): SiLU()\n",
      "    )\n",
      "    (1): EfficientConvBlock(\n",
      "      (expand): Sequential(\n",
      "        (0): Conv2d(256, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): GroupNorm(8, 2048, eps=1e-05, affine=True)\n",
      "        (2): SiLU()\n",
      "      )\n",
      "      (depthwise): Sequential(\n",
      "        (0): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048, bias=False)\n",
      "        (1): GroupNorm(8, 2048, eps=1e-05, affine=True)\n",
      "        (2): SiLU()\n",
      "      )\n",
      "      (se): SqueezeExcitationBlock(\n",
      "        (global_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc1): Linear(in_features=2048, out_features=128, bias=True)\n",
      "        (fc2): Linear(in_features=128, out_features=2048, bias=True)\n",
      "        (swish): SiLU()\n",
      "        (sigmoid): Sigmoid()\n",
      "      )\n",
      "      (project): Sequential(\n",
      "        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (global_avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (global_max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "  (classifier_head): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=768, bias=False)\n",
      "    (1): GroupNorm(1, 768, eps=1e-05, affine=True)\n",
      "    (2): SiLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=768, out_features=384, bias=False)\n",
      "    (5): GroupNorm(1, 384, eps=1e-05, affine=True)\n",
      "    (6): SiLU()\n",
      "    (7): Dropout(p=0.4, inplace=False)\n",
      "  )\n",
      "  (mlp): MLP(\n",
      "    (layers): Sequential(\n",
      "      (0): Linear(in_features=384, out_features=2048, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Dropout(p=0.5, inplace=False)\n",
      "      (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "      (4): GELU(approximate='none')\n",
      "      (5): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_classifier): Linear(in_features=1024, out_features=100, bias=True)\n",
      ")\n",
      "\n",
      "Total trainable parameters: 9,810,050\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/10\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|█████████▍| 296/313 [02:23<00:08,  2.06it/s, Loss=4.798, Acc=1.05%]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3327499459.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nStarting training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Evaluate on test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-178448850.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, num_epochs, device)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;31m# Validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-178448850.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create data loaders\n",
    "# Split training data for validation\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_subset, val_subset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# Create model\n",
    "model = CNNClassifier(num_classes=num_classes, mlp_head_units=mlp_head_units).to(device)\n",
    "\n",
    "# Print model summary\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Model Architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal trainable parameters: {count_parameters(model):,}\")\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nStarting training...\")\n",
    "model, history = train_model(model, train_loader, val_loader, num_epochs, device)\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "criterion = LabelSmoothingCrossEntropy(smoothing=0.0)  # No smoothing for evaluation\n",
    "test_loss, test_acc, test_top5_acc, test_top10_acc = validate(model, test_loader, criterion, device)\n",
    "\n",
    "print(f\"\\nFinal Test Results:\")\n",
    "print(f\"Test accuracy: {test_acc:.2f}%\")\n",
    "print(f\"Test top 5 accuracy: {test_top5_acc:.2f}%\")\n",
    "print(f\"Test top 10 accuracy: {test_top10_acc:.2f}%\")\n",
    "print(f\"Test loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b398bf",
   "metadata": {
    "id": "f7b398bf"
   },
   "source": [
    "## Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4598e65b",
   "metadata": {
    "executionInfo": {
     "elapsed": 162137,
     "status": "aborted",
     "timestamp": 1760816546028,
     "user": {
      "displayName": "Asadullah Hil Galib",
      "userId": "02825503865158832131"
     },
     "user_tz": -360
    },
    "id": "4598e65b"
   },
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"Plot comprehensive training history\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    # Plot loss\n",
    "    axes[0, 0].plot(history['train_loss'], label='Training Loss')\n",
    "    axes[0, 0].plot(history['val_loss'], label='Validation Loss')\n",
    "    axes[0, 0].set_title('Model Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "\n",
    "    # Plot accuracy\n",
    "    axes[0, 1].plot(history['train_acc'], label='Training Accuracy')\n",
    "    axes[0, 1].plot(history['val_acc'], label='Validation Accuracy')\n",
    "    axes[0, 1].set_title('Model Accuracy')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "\n",
    "    # Plot top-5 accuracy\n",
    "    axes[1, 0].plot(history['val_top5_acc'], label='Validation Top-5 Accuracy')\n",
    "    axes[1, 0].plot(history['val_top10_acc'], label='Validation Top-10 Accuracy')\n",
    "    axes[1, 0].set_title('Model Top-K Accuracy')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Accuracy (%)')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True)\n",
    "\n",
    "    # Plot learning rate\n",
    "    axes[1, 1].plot(history['lr'])\n",
    "    axes[1, 1].set_title('Learning Rate Schedule')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Learning Rate')\n",
    "    axes[1, 1].set_yscale('log')\n",
    "    axes[1, 1].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the training history\n",
    "plot_training_history(history)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
