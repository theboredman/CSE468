{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f92af4be",
   "metadata": {
    "id": "f92af4be"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/theboredman/CSE468/blob/main/Quiz_1/CNN/Using_CNN_CIFAR100_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8856f835",
   "metadata": {
    "id": "8856f835"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48037d5c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11481,
     "status": "ok",
     "timestamp": 1761408396673,
     "user": {
      "displayName": "Asadullah Hil Galib",
      "userId": "02825503865158832131"
     },
     "user_tz": -360
    },
    "id": "48037d5c",
    "outputId": "949a2c82-873c-4364-e16e-bc9d59c2df8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR100\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550841e0",
   "metadata": {
    "id": "550841e0"
   },
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a38ffc11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18419,
     "status": "ok",
     "timestamp": 1761408415101,
     "user": {
      "displayName": "Asadullah Hil Galib",
      "userId": "02825503865158832131"
     },
     "user_tz": -360
    },
    "id": "a38ffc11",
    "outputId": "8b4a5bdb-b085-422e-b383-eac301ceb92b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169M/169M [00:13<00:00, 12.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 50000\n",
      "Test samples: 10000\n"
     ]
    }
   ],
   "source": [
    "num_classes = 100\n",
    "input_shape = (3, 32, 32)  # PyTorch uses (C, H, W) format\n",
    "\n",
    "from torchvision.transforms import AutoAugment, AutoAugmentPolicy, RandAugment\n",
    "\n",
    "# Define transforms for training\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((80, 80)),\n",
    "    transforms.RandomCrop(72, padding=4),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "\n",
    "    AutoAugment(policy=AutoAugmentPolicy.CIFAR10),\n",
    "\n",
    "    RandAugment(num_ops=2, magnitude=9),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761]),\n",
    "    # Random Erasing\n",
    "    transforms.RandomErasing(p=0.25, scale=(0.02, 0.33), ratio=(0.3, 3.3)),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((72, 72)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
    "])\n",
    "\n",
    "# Load CIFAR-100 dataset\n",
    "train_dataset = CIFAR100(root='./data', train=True, download=True, transform=train_transform)\n",
    "test_dataset = CIFAR100(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e76953",
   "metadata": {
    "id": "98e76953"
   },
   "source": [
    "## Configure the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ef88639",
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1761408415113,
     "user": {
      "displayName": "Asadullah Hil Galib",
      "userId": "02825503865158832131"
     },
     "user_tz": -360
    },
    "id": "8ef88639"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.05\n",
    "batch_size = 256\n",
    "num_epochs = 50\n",
    "image_size = 72\n",
    "\n",
    "mlp_head_units = [2048, 1024]\n",
    "\n",
    "# Advanced hyperparameters optimized for speed\n",
    "initial_learning_rate = 0.003\n",
    "label_smoothing = 0.1\n",
    "dropout_rate = 0.3\n",
    "warmup_epochs = 3\n",
    "min_lr = 1e-6\n",
    "mixup_alpha = 0.4\n",
    "cutmix_alpha = 1.0\n",
    "mixup_prob = 0.8\n",
    "stochastic_depth_rate = 0.3\n",
    "gradient_clip_norm = 1.0\n",
    "ema_decay = 0.9998\n",
    "accumulation_steps = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fd44c9",
   "metadata": {
    "id": "63fd44c9"
   },
   "source": [
    "## Implement multilayer perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89fa985a",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1761408415129,
     "user": {
      "displayName": "Asadullah Hil Galib",
      "userId": "02825503865158832131"
     },
     "user_tz": -360
    },
    "id": "89fa985a"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_units, dropout_rate):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "\n",
    "        for units in hidden_units:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, units),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_dim = units\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ad32cd",
   "metadata": {
    "id": "74ad32cd"
   },
   "source": [
    "## CNN Architecture\n",
    "\n",
    "We'll build a CNN with convolutional layers for feature extraction followed by dense layers for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c47539a3",
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1761408415150,
     "user": {
      "displayName": "Asadullah Hil Galib",
      "userId": "02825503865158832131"
     },
     "user_tz": -360
    },
    "id": "c47539a3"
   },
   "outputs": [],
   "source": [
    "class StochasticDepth(nn.Module):\n",
    "    \"\"\"Stochastic Depth (Drop Path) for better regularization\"\"\"\n",
    "    def __init__(self, drop_prob=0.0):\n",
    "        super(StochasticDepth, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.training or self.drop_prob == 0.:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_()\n",
    "        return x.div(keep_prob) * random_tensor\n",
    "\n",
    "class SqueezeExcitationBlock(nn.Module):\n",
    "    \"\"\"Enhanced Squeeze-and-Excitation block\"\"\"\n",
    "    def __init__(self, channels, ratio=16):\n",
    "        super(SqueezeExcitationBlock, self).__init__()\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        reduced_channels = max(1, channels // ratio)\n",
    "        self.fc1 = nn.Linear(channels, reduced_channels)\n",
    "        self.fc2 = nn.Linear(reduced_channels, channels)\n",
    "        self.swish = nn.SiLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        se = self.global_pool(x).view(b, c)\n",
    "        se = self.swish(self.fc1(se))\n",
    "        se = self.sigmoid(self.fc2(se)).view(b, c, 1, 1)\n",
    "        return x * se\n",
    "\n",
    "class FusedMBConv(nn.Module):\n",
    "    \"\"\"Fused MBConv from EfficientNetV2 - faster and more efficient\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, expansion_factor=4, drop_path=0.0):\n",
    "        super(FusedMBConv, self).__init__()\n",
    "        self.stride = stride\n",
    "        self.use_residual = stride == 1 and in_channels == out_channels\n",
    "        expanded_channels = in_channels * expansion_factor\n",
    "\n",
    "        # Fused expansion + depthwise\n",
    "        self.fused_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, expanded_channels, kernel_size, stride, padding=kernel_size//2, bias=False),\n",
    "            nn.GroupNorm(16, expanded_channels),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "        # SE block\n",
    "        self.se = SqueezeExcitationBlock(expanded_channels, ratio=4)\n",
    "\n",
    "        # Projection\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv2d(expanded_channels, out_channels, 1, bias=False),\n",
    "            nn.GroupNorm(16, out_channels)\n",
    "        )\n",
    "\n",
    "        # Stochastic depth\n",
    "        self.drop_path = StochasticDepth(drop_path) if drop_path > 0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        x = self.fused_conv(x)\n",
    "        x = self.se(x)\n",
    "        x = self.project(x)\n",
    "\n",
    "        if self.use_residual:\n",
    "            x = self.drop_path(x)\n",
    "            x = x + identity\n",
    "\n",
    "        return x\n",
    "\n",
    "class ImprovedResidualBlock(nn.Module):\n",
    "    \"\"\"Enhanced residual block with Stochastic Depth\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, use_se=True, drop_path=0.0):\n",
    "        super(ImprovedResidualBlock, self).__init__()\n",
    "        self.use_se = use_se\n",
    "        self.use_residual = stride == 1 and in_channels == out_channels\n",
    "\n",
    "        # First conv layer with Group Normalization\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=kernel_size//2, bias=False)\n",
    "        self.gn1 = nn.GroupNorm(16, out_channels)\n",
    "\n",
    "        # Second conv layer\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, 1, padding=kernel_size//2, bias=False)\n",
    "        self.gn2 = nn.GroupNorm(16, out_channels)\n",
    "\n",
    "        # Squeeze-and-Excitation\n",
    "        if use_se:\n",
    "            self.se = SqueezeExcitationBlock(out_channels, ratio=4)\n",
    "\n",
    "        # Shortcut connection\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1, stride, bias=False),\n",
    "                nn.GroupNorm(16, out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "        self.swish = nn.SiLU()\n",
    "        self.drop_path = StochasticDepth(drop_path) if drop_path > 0 and self.use_residual else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.shortcut(x)\n",
    "\n",
    "        out = self.swish(self.gn1(self.conv1(x)))\n",
    "        out = self.gn2(self.conv2(out))\n",
    "\n",
    "        if self.use_se:\n",
    "            out = self.se(out)\n",
    "\n",
    "        out = self.drop_path(out)\n",
    "        out = out + residual\n",
    "        out = self.swish(out)\n",
    "        return out\n",
    "\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=100, mlp_head_units=[2048, 1024], drop_path_rate=0.2):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "\n",
    "        # Calculate drop path rates (linearly increasing)\n",
    "        total_blocks = 16\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, total_blocks)]\n",
    "        block_idx = 0\n",
    "\n",
    "        # Enhanced Stem with more capacity\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1, bias=False),\n",
    "            nn.GroupNorm(8, 32),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(32, 48, 3, padding=1, bias=False),\n",
    "            nn.GroupNorm(8, 48),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "        # Stage 1: FusedMBConv blocks (efficient early stages)\n",
    "        self.stage1 = nn.Sequential(\n",
    "            FusedMBConv(48, 64, stride=1, expansion_factor=4, drop_path=dpr[block_idx]),\n",
    "            FusedMBConv(64, 64, stride=1, expansion_factor=4, drop_path=dpr[block_idx+1]),\n",
    "            FusedMBConv(64, 64, stride=2, expansion_factor=4, drop_path=dpr[block_idx+2]),\n",
    "            nn.Dropout2d(0.1)\n",
    "        )\n",
    "        block_idx += 3\n",
    "\n",
    "        # Stage 2: Mixed block types\n",
    "        self.stage2 = nn.Sequential(\n",
    "            FusedMBConv(64, 96, stride=1, expansion_factor=4, drop_path=dpr[block_idx]),\n",
    "            ImprovedResidualBlock(96, 96, stride=1, use_se=True, drop_path=dpr[block_idx+1]),\n",
    "            FusedMBConv(96, 96, stride=2, expansion_factor=4, drop_path=dpr[block_idx+2]),\n",
    "            nn.Dropout2d(0.15)\n",
    "        )\n",
    "        block_idx += 3\n",
    "\n",
    "        # Stage 3: Deeper feature extraction\n",
    "        self.stage3 = nn.Sequential(\n",
    "            ImprovedResidualBlock(96, 144, stride=1, use_se=True, drop_path=dpr[block_idx]),\n",
    "            FusedMBConv(144, 144, stride=1, expansion_factor=6, drop_path=dpr[block_idx+1]),\n",
    "            ImprovedResidualBlock(144, 144, stride=1, use_se=True, drop_path=dpr[block_idx+2]),\n",
    "            nn.AvgPool2d(2),\n",
    "            nn.Dropout2d(0.2)\n",
    "        )\n",
    "        block_idx += 3\n",
    "\n",
    "        # Stage 4: High-level features\n",
    "        self.stage4 = nn.Sequential(\n",
    "            FusedMBConv(144, 192, stride=1, expansion_factor=6, drop_path=dpr[block_idx]),\n",
    "            ImprovedResidualBlock(192, 192, stride=1, use_se=True, drop_path=dpr[block_idx+1]),\n",
    "            FusedMBConv(192, 192, stride=1, expansion_factor=6, drop_path=dpr[block_idx+2]),\n",
    "            ImprovedResidualBlock(192, 192, stride=1, use_se=True, drop_path=dpr[block_idx+3]),\n",
    "            nn.Dropout2d(0.25)\n",
    "        )\n",
    "        block_idx += 4\n",
    "\n",
    "        # Stage 5: Final high-capacity features\n",
    "        self.stage5 = nn.Sequential(\n",
    "            ImprovedResidualBlock(192, 256, stride=1, use_se=True, drop_path=dpr[block_idx]),\n",
    "            FusedMBConv(256, 256, stride=1, expansion_factor=8, drop_path=dpr[block_idx+1]),\n",
    "            ImprovedResidualBlock(256, 256, stride=1, use_se=True, drop_path=dpr[block_idx+2])\n",
    "        )\n",
    "\n",
    "        # Multi-scale feature aggregation\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.global_max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        # Enhanced classifier with GeM pooling information\n",
    "        self.classifier_head = nn.Sequential(\n",
    "            nn.Linear(512, 896, bias=False),  # 256*2 from pooling + extra capacity\n",
    "            nn.GroupNorm(1, 896),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(896, 512, bias=False),\n",
    "            nn.GroupNorm(1, 512),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.4)\n",
    "        )\n",
    "\n",
    "        # MLP classification head\n",
    "        self.mlp = MLP(512, mlp_head_units, 0.5)\n",
    "\n",
    "        # Final classification layer\n",
    "        self.final_classifier = nn.Linear(mlp_head_units[-1], num_classes)\n",
    "\n",
    "        # Initialize weights with better strategy\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, (nn.GroupNorm, nn.BatchNorm2d)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Stem\n",
    "        x = self.stem(x)\n",
    "\n",
    "        # Progressive stages\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "        x = self.stage5(x)\n",
    "\n",
    "        # Multi-scale global pooling\n",
    "        gap = self.global_avg_pool(x).flatten(1)\n",
    "        gmp = self.global_max_pool(x).flatten(1)\n",
    "        x = torch.cat([gap, gmp], dim=1)\n",
    "\n",
    "        # Classification head\n",
    "        x = self.classifier_head(x)\n",
    "        x = self.mlp(x)\n",
    "        x = self.final_classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fa4748",
   "metadata": {
    "id": "10fa4748"
   },
   "source": [
    "## Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d1fb4b4",
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1761408415183,
     "user": {
      "displayName": "Asadullah Hil Galib",
      "userId": "02825503865158832131"
     },
     "user_tz": -360
    },
    "id": "6d1fb4b4"
   },
   "outputs": [],
   "source": [
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    \"\"\"Label smoothing cross entropy loss\"\"\"\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        log_prob = F.log_softmax(pred, dim=-1)\n",
    "        weight = pred.new_ones(pred.size()) * self.smoothing / (pred.size(-1) - 1.)\n",
    "        weight.scatter_(-1, target.unsqueeze(-1), (1. - self.smoothing))\n",
    "        loss = (-weight * log_prob).sum(dim=-1).mean()\n",
    "        return loss\n",
    "\n",
    "class ModelEMA:\n",
    "    \"\"\"Exponential Moving Average of model parameters\"\"\"\n",
    "    def __init__(self, model, decay=0.9999):\n",
    "        self.module = type(model)(num_classes=model.final_classifier.out_features,\n",
    "                                   mlp_head_units=[2048, 1024],\n",
    "                                   drop_path_rate=stochastic_depth_rate).to(next(model.parameters()).device)\n",
    "        self.module.load_state_dict(model.state_dict())\n",
    "        self.module.eval()\n",
    "        self.decay = decay\n",
    "        self.device = next(model.parameters()).device\n",
    "\n",
    "    def update(self, model):\n",
    "        with torch.no_grad():\n",
    "            for ema_param, model_param in zip(self.module.parameters(), model.parameters()):\n",
    "                ema_param.data.mul_(self.decay).add_(model_param.data, alpha=1 - self.decay)\n",
    "\n",
    "def mixup_data(x, y, alpha=0.2):\n",
    "    \"\"\"Apply Mixup augmentation\"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def cutmix_data(x, y, alpha=1.0):\n",
    "    \"\"\"Apply CutMix augmentation\"\"\"\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "\n",
    "    _, _, H, W = x.size()\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "\n",
    "    # Uniform sampling\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    x[:, :, bby1:bby2, bbx1:bbx2] = x[index, :, bby1:bby2, bbx1:bbx2]\n",
    "\n",
    "    # Adjust lambda to match pixel ratio\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (W * H))\n",
    "    y_a, y_b = y, y[index]\n",
    "    return x, y_a, y_b, lam\n",
    "\n",
    "def calculate_accuracy(outputs, targets, topk=(1, 5, 10)):\n",
    "    \"\"\"Calculate top-k accuracy\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = targets.size(0)\n",
    "\n",
    "        _, pred = outputs.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(targets.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device, ema=None, use_mixup=True, scaler=None):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    pbar = tqdm(train_loader, desc='Training')\n",
    "    for batch_idx, (data, target) in enumerate(pbar):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Mixed precision training\n",
    "        with torch.amp.autocast('cuda', enabled=(scaler is not None)):\n",
    "            # Apply Mixup or CutMix\n",
    "            if use_mixup and np.random.rand() < mixup_prob:\n",
    "                if np.random.rand() < 0.5:\n",
    "                    data, target_a, target_b, lam = mixup_data(data, target, mixup_alpha)\n",
    "                else:\n",
    "                    data, target_a, target_b, lam = cutmix_data(data, target, cutmix_alpha)\n",
    "\n",
    "                output = model(data)\n",
    "                loss = lam * criterion(output, target_a) + (1 - lam) * criterion(output, target_b)\n",
    "            else:\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "\n",
    "            # Gradient accumulation\n",
    "            loss = loss / accumulation_steps\n",
    "\n",
    "        # Backward pass with mixed precision\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        # Update weights every accumulation_steps\n",
    "        if (batch_idx + 1) % accumulation_steps == 0:\n",
    "            if scaler is not None:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip_norm)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip_norm)\n",
    "                optimizer.step()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Update EMA\n",
    "            if ema is not None:\n",
    "                ema.update(model)\n",
    "\n",
    "        running_loss += loss.item() * accumulation_steps\n",
    "        _, predicted = output.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            'Loss': f'{running_loss/(batch_idx+1):.3f}',\n",
    "            'Acc': f'{100.*correct/total:.2f}%'\n",
    "        })\n",
    "\n",
    "    return running_loss / len(train_loader), 100. * correct / total\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    top1_correct = 0\n",
    "    top5_correct = 0\n",
    "    top10_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader, desc='Validation')\n",
    "        for data, target in pbar:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            val_loss += criterion(output, target).item()\n",
    "\n",
    "            # Calculate top-k accuracies\n",
    "            acc1, acc5, acc10 = calculate_accuracy(output, target, topk=(1, 5, 10))\n",
    "            top1_correct += acc1.item() * target.size(0) / 100\n",
    "            top5_correct += acc5.item() * target.size(0) / 100\n",
    "            top10_correct += acc10.item() * target.size(0) / 100\n",
    "            total += target.size(0)\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                'Loss': f'{val_loss/total:.3f}',\n",
    "                'Top1': f'{100.*top1_correct/total:.2f}%',\n",
    "                'Top5': f'{100.*top5_correct/total:.2f}%'\n",
    "            })\n",
    "\n",
    "    return (val_loss / len(val_loader),\n",
    "            100. * top1_correct / total,\n",
    "            100. * top5_correct / total,\n",
    "            100. * top10_correct / total)\n",
    "\n",
    "def warmup_cosine_annealing_lr(epoch, warmup_epochs, total_epochs, initial_lr, min_lr):\n",
    "    \"\"\"Warmup + Cosine annealing learning rate schedule\"\"\"\n",
    "    if epoch < warmup_epochs:\n",
    "        return initial_lr * (epoch + 1) / warmup_epochs\n",
    "    else:\n",
    "        progress = (epoch - warmup_epochs) / (total_epochs - warmup_epochs)\n",
    "        return min_lr + (initial_lr - min_lr) * 0.5 * (1 + np.cos(np.pi * progress))\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs, device):\n",
    "    # Loss function with label smoothing\n",
    "    criterion = LabelSmoothingCrossEntropy(smoothing=label_smoothing)\n",
    "\n",
    "    # Optimizer with higher LR for faster convergence\n",
    "    optimizer = optim.AdamW(model.parameters(),\n",
    "                           lr=initial_learning_rate,\n",
    "                           weight_decay=weight_decay,\n",
    "                           betas=(0.9, 0.999),\n",
    "                           eps=1e-8)\n",
    "\n",
    "    # OneCycleLR for faster convergence - best scheduler for limited epochs\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=initial_learning_rate,\n",
    "        epochs=num_epochs,\n",
    "        steps_per_epoch=len(train_loader) // accumulation_steps,\n",
    "        pct_start=0.1,  # 10% warmup\n",
    "        anneal_strategy='cos',\n",
    "        div_factor=25.0,  # initial_lr = max_lr / 25\n",
    "        final_div_factor=1e4  # min_lr = initial_lr / 1e4\n",
    "    )\n",
    "\n",
    "    # Initialize EMA\n",
    "    ema = ModelEMA(model, decay=ema_decay)\n",
    "\n",
    "    # Mixed precision training for faster computation\n",
    "    scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None\n",
    "\n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'val_loss': [], 'val_acc': [], 'val_top5_acc': [], 'val_top10_acc': [],\n",
    "        'lr': []\n",
    "    }\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    patience = 15  # Early stopping patience\n",
    "\n",
    "    print(f\"\\n⚡ Fast Training Mode: {num_epochs} epochs with aggressive optimization\")\n",
    "    print(f\"Effective batch size: {batch_size * accumulation_steps}\")\n",
    "    print(f\"Using {'mixed precision' if scaler else 'full precision'} training\\n\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 50)\n",
    "\n",
    "        # Training\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer,\n",
    "                                           device, ema=ema, use_mixup=True, scaler=scaler)\n",
    "\n",
    "        # Validation with EMA model\n",
    "        val_loss, val_acc, val_top5_acc, val_top10_acc = validate(ema.module, val_loader, criterion, device)\n",
    "\n",
    "        # Update learning rate (OneCycleLR steps per batch, not per epoch)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_top5_acc'].append(val_top5_acc)\n",
    "        history['val_top10_acc'].append(val_top10_acc)\n",
    "        history['lr'].append(current_lr)\n",
    "\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, Val Top5: {val_top5_acc:.2f}%, Val Top10: {val_top10_acc:.2f}%')\n",
    "        print(f'Learning Rate: {current_lr:.6f}')\n",
    "\n",
    "        # Early stopping and model checkpoint\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'ema_state_dict': ema.module.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_acc': val_acc,\n",
    "            }, 'best_model.pth')\n",
    "            print(f'✓ New best validation accuracy: {best_val_acc:.2f}%')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f'⚠ Early stopping at epoch {epoch+1}')\n",
    "            break\n",
    "\n",
    "    # Load best EMA model\n",
    "    checkpoint = torch.load('best_model.pth')\n",
    "    ema.module.load_state_dict(checkpoint['ema_state_dict'])\n",
    "\n",
    "    return ema.module, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3647fa",
   "metadata": {
    "id": "3e3647fa"
   },
   "source": [
    "## Compile, train, and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6843a450",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6843a450",
    "outputId": "5a88ddea-82f2-466e-ccd3-dc6674d63a67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CNN MODEL FOR CIFAR-100\n",
      "======================================================================\n",
      "\n",
      "Model Configuration:\n",
      "  • Total trainable parameters: 23,442,308\n",
      "  • Batch size: 256 (effective: 512 with gradient accumulation)\n",
      "  • Training epochs: 50\n",
      "  • Initial learning rate: 0.003\n",
      "  • Weight decay: 0.05\n",
      "  • Gradient accumulation steps: 2\n",
      "\n",
      "Fast Training Optimizations:\n",
      "  ⚡ OneCycleLR scheduler (superconvergence)\n",
      "  ⚡ Mixed precision training (FP16)\n",
      "  ⚡ Gradient accumulation (2x effective batch)\n",
      "  ⚡ Larger batch size for stability\n",
      "  ⚡ Higher learning rate (0.003)\n",
      "  ⚡ Aggressive augmentation (mixup_prob=0.8)\n",
      "\n",
      "Regularization:\n",
      "  • Stochastic depth: 0.3\n",
      "  • EMA decay: 0.9998\n",
      "  • Label smoothing: 0.1\n",
      "  • Mixup alpha: 0.4\n",
      "  • CutMix alpha: 1.0\n",
      "\n",
      "Augmentation Strategy:\n",
      "  • AutoAugment (CIFAR10 policy)\n",
      "  • RandAugment (2 ops, magnitude 9)\n",
      "  • Random Erasing (p=0.25)\n",
      "  • Mixup/CutMix (p=0.8)\n",
      "\n",
      "Architecture Features:\n",
      "  • FusedMBConv blocks (EfficientNetV2)\n",
      "  • Stochastic Depth regularization\n",
      "  • Squeeze-and-Excitation attention\n",
      "  • Multi-scale global pooling\n",
      "  • Exponential Moving Average\n",
      "======================================================================\n",
      "Expected completion: ~50 epochs with early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3365095473.py:219: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚡ Fast Training Mode: 50 epochs with aggressive optimization\n",
      "Effective batch size: 512\n",
      "Using mixed precision training\n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [02:48<00:00,  1.04it/s, Loss=4.611, Acc=0.95%]\n",
      "Validation: 100%|██████████| 20/20 [00:06<00:00,  3.18it/s, Loss=0.018, Top1=0.88%, Top5=5.06%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.6113, Train Acc: 0.95%\n",
      "Val Loss: 4.6065, Val Acc: 0.88%, Val Top5: 5.06%, Val Top10: 9.78%\n",
      "Learning Rate: 0.000120\n",
      "✓ New best validation accuracy: 0.88%\n",
      "\n",
      "Epoch 2/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [02:46<00:00,  1.06it/s, Loss=4.607, Acc=1.06%]\n",
      "Validation: 100%|██████████| 20/20 [00:06<00:00,  3.28it/s, Loss=0.018, Top1=1.04%, Top5=5.04%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.6074, Train Acc: 1.06%\n",
      "Val Loss: 4.6062, Val Acc: 1.04%, Val Top5: 5.04%, Val Top10: 9.54%\n",
      "Learning Rate: 0.000120\n",
      "✓ New best validation accuracy: 1.04%\n",
      "\n",
      "Epoch 3/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [02:46<00:00,  1.06it/s, Loss=4.607, Acc=1.00%]\n",
      "Validation: 100%|██████████| 20/20 [00:06<00:00,  3.29it/s, Loss=0.018, Top1=0.96%, Top5=4.74%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.6069, Train Acc: 1.00%\n",
      "Val Loss: 4.6063, Val Acc: 0.96%, Val Top5: 4.74%, Val Top10: 9.58%\n",
      "Learning Rate: 0.000120\n",
      "\n",
      "Epoch 4/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [02:46<00:00,  1.06it/s, Loss=4.606, Acc=0.94%]\n",
      "Validation: 100%|██████████| 20/20 [00:06<00:00,  3.30it/s, Loss=0.018, Top1=0.84%, Top5=4.52%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.6064, Train Acc: 0.94%\n",
      "Val Loss: 4.6063, Val Acc: 0.84%, Val Top5: 4.52%, Val Top10: 9.28%\n",
      "Learning Rate: 0.000120\n",
      "\n",
      "Epoch 5/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [02:46<00:00,  1.06it/s, Loss=4.606, Acc=0.98%]\n",
      "Validation: 100%|██████████| 20/20 [00:06<00:00,  3.29it/s, Loss=0.018, Top1=1.02%, Top5=4.50%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.6057, Train Acc: 0.98%\n",
      "Val Loss: 4.6063, Val Acc: 1.02%, Val Top5: 4.50%, Val Top10: 8.96%\n",
      "Learning Rate: 0.000120\n",
      "\n",
      "Epoch 6/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [02:46<00:00,  1.06it/s, Loss=4.602, Acc=1.16%]\n",
      "Validation: 100%|██████████| 20/20 [00:06<00:00,  3.28it/s, Loss=0.018, Top1=0.86%, Top5=4.58%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.6019, Train Acc: 1.16%\n",
      "Val Loss: 4.6060, Val Acc: 0.86%, Val Top5: 4.58%, Val Top10: 9.60%\n",
      "Learning Rate: 0.000120\n",
      "\n",
      "Epoch 7/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [02:46<00:00,  1.06it/s, Loss=4.566, Acc=1.53%]\n",
      "Validation: 100%|██████████| 20/20 [00:06<00:00,  3.29it/s, Loss=0.018, Top1=0.64%, Top5=5.06%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.5664, Train Acc: 1.53%\n",
      "Val Loss: 4.6063, Val Acc: 0.64%, Val Top5: 5.06%, Val Top10: 9.20%\n",
      "Learning Rate: 0.000120\n",
      "\n",
      "Epoch 8/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [02:46<00:00,  1.06it/s, Loss=4.532, Acc=1.54%]\n",
      "Validation: 100%|██████████| 20/20 [00:06<00:00,  3.30it/s, Loss=0.018, Top1=1.04%, Top5=4.46%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.5317, Train Acc: 1.54%\n",
      "Val Loss: 4.6064, Val Acc: 1.04%, Val Top5: 4.46%, Val Top10: 9.54%\n",
      "Learning Rate: 0.000120\n",
      "✓ New best validation accuracy: 1.04%\n",
      "\n",
      "Epoch 9/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [02:46<00:00,  1.06it/s, Loss=4.511, Acc=1.87%]\n",
      "Validation: 100%|██████████| 20/20 [00:06<00:00,  3.28it/s, Loss=0.018, Top1=0.96%, Top5=4.72%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.5110, Train Acc: 1.87%\n",
      "Val Loss: 4.6060, Val Acc: 0.96%, Val Top5: 4.72%, Val Top10: 9.08%\n",
      "Learning Rate: 0.000120\n",
      "\n",
      "Epoch 10/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [02:46<00:00,  1.06it/s, Loss=4.487, Acc=1.92%]\n",
      "Validation: 100%|██████████| 20/20 [00:06<00:00,  3.30it/s, Loss=0.018, Top1=0.92%, Top5=4.70%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.4871, Train Acc: 1.92%\n",
      "Val Loss: 4.6058, Val Acc: 0.92%, Val Top5: 4.70%, Val Top10: 9.34%\n",
      "Learning Rate: 0.000120\n",
      "\n",
      "Epoch 11/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [02:46<00:00,  1.06it/s, Loss=4.478, Acc=2.17%]\n",
      "Validation: 100%|██████████| 20/20 [00:06<00:00,  3.29it/s, Loss=0.018, Top1=1.02%, Top5=5.14%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.4781, Train Acc: 2.17%\n",
      "Val Loss: 4.6053, Val Acc: 1.02%, Val Top5: 5.14%, Val Top10: 9.22%\n",
      "Learning Rate: 0.000120\n",
      "\n",
      "Epoch 12/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [02:46<00:00,  1.06it/s, Loss=4.456, Acc=2.35%]\n",
      "Validation: 100%|██████████| 20/20 [00:06<00:00,  3.29it/s, Loss=0.018, Top1=1.04%, Top5=4.96%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.4561, Train Acc: 2.35%\n",
      "Val Loss: 4.6052, Val Acc: 1.04%, Val Top5: 4.96%, Val Top10: 9.64%\n",
      "Learning Rate: 0.000120\n",
      "\n",
      "Epoch 13/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [02:46<00:00,  1.06it/s, Loss=4.440, Acc=2.53%]\n",
      "Validation: 100%|██████████| 20/20 [00:06<00:00,  3.29it/s, Loss=0.018, Top1=1.14%, Top5=4.84%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.4396, Train Acc: 2.53%\n",
      "Val Loss: 4.6046, Val Acc: 1.14%, Val Top5: 4.84%, Val Top10: 10.02%\n",
      "Learning Rate: 0.000120\n",
      "✓ New best validation accuracy: 1.14%\n",
      "\n",
      "Epoch 14/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [02:46<00:00,  1.06it/s, Loss=4.416, Acc=2.78%]\n",
      "Validation: 100%|██████████| 20/20 [00:06<00:00,  3.30it/s, Loss=0.018, Top1=1.18%, Top5=5.16%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.4164, Train Acc: 2.78%\n",
      "Val Loss: 4.6042, Val Acc: 1.18%, Val Top5: 5.16%, Val Top10: 10.58%\n",
      "Learning Rate: 0.000120\n",
      "✓ New best validation accuracy: 1.18%\n",
      "\n",
      "Epoch 15/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [02:46<00:00,  1.06it/s, Loss=4.385, Acc=3.05%]\n",
      "Validation: 100%|██████████| 20/20 [00:06<00:00,  3.29it/s, Loss=0.018, Top1=1.16%, Top5=4.96%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.3846, Train Acc: 3.05%\n",
      "Val Loss: 4.6033, Val Acc: 1.16%, Val Top5: 4.96%, Val Top10: 10.20%\n",
      "Learning Rate: 0.000120\n",
      "\n",
      "Epoch 16/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [02:46<00:00,  1.06it/s, Loss=4.371, Acc=3.42%]\n",
      "Validation: 100%|██████████| 20/20 [00:06<00:00,  3.30it/s, Loss=0.018, Top1=1.18%, Top5=5.34%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.3708, Train Acc: 3.42%\n",
      "Val Loss: 4.6028, Val Acc: 1.18%, Val Top5: 5.34%, Val Top10: 10.68%\n",
      "Learning Rate: 0.000120\n",
      "✓ New best validation accuracy: 1.18%\n",
      "\n",
      "Epoch 17/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [02:46<00:00,  1.06it/s, Loss=4.363, Acc=3.46%]\n",
      "Validation: 100%|██████████| 20/20 [00:06<00:00,  3.29it/s, Loss=0.018, Top1=1.26%, Top5=5.68%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.3626, Train Acc: 3.46%\n",
      "Val Loss: 4.6016, Val Acc: 1.26%, Val Top5: 5.68%, Val Top10: 10.60%\n",
      "Learning Rate: 0.000120\n",
      "✓ New best validation accuracy: 1.26%\n",
      "\n",
      "Epoch 18/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [02:46<00:00,  1.06it/s, Loss=4.355, Acc=3.75%]\n",
      "Validation: 100%|██████████| 20/20 [00:06<00:00,  3.30it/s, Loss=0.018, Top1=1.18%, Top5=5.32%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.3550, Train Acc: 3.75%\n",
      "Val Loss: 4.5999, Val Acc: 1.18%, Val Top5: 5.32%, Val Top10: 11.16%\n",
      "Learning Rate: 0.000120\n",
      "\n",
      "Epoch 19/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [02:46<00:00,  1.06it/s, Loss=4.310, Acc=4.51%]\n",
      "Validation: 100%|██████████| 20/20 [00:06<00:00,  3.30it/s, Loss=0.018, Top1=1.06%, Top5=5.86%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.3104, Train Acc: 4.51%\n",
      "Val Loss: 4.5993, Val Acc: 1.06%, Val Top5: 5.86%, Val Top10: 11.34%\n",
      "Learning Rate: 0.000120\n",
      "\n",
      "Epoch 20/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [02:46<00:00,  1.06it/s, Loss=4.319, Acc=4.01%]\n",
      "Validation: 100%|██████████| 20/20 [00:06<00:00,  3.30it/s, Loss=0.018, Top1=1.08%, Top5=5.96%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.3188, Train Acc: 4.01%\n",
      "Val Loss: 4.5983, Val Acc: 1.08%, Val Top5: 5.96%, Val Top10: 11.20%\n",
      "Learning Rate: 0.000120\n",
      "\n",
      "Epoch 21/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [02:46<00:00,  1.06it/s, Loss=4.295, Acc=4.46%]\n",
      "Validation: 100%|██████████| 20/20 [00:06<00:00,  3.29it/s, Loss=0.018, Top1=1.10%, Top5=6.24%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.2945, Train Acc: 4.46%\n",
      "Val Loss: 4.5963, Val Acc: 1.10%, Val Top5: 6.24%, Val Top10: 11.96%\n",
      "Learning Rate: 0.000120\n",
      "\n",
      "Epoch 22/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [02:46<00:00,  1.06it/s, Loss=4.270, Acc=4.66%]\n",
      "Validation: 100%|██████████| 20/20 [00:06<00:00,  3.29it/s, Loss=0.018, Top1=1.02%, Top5=5.98%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.2696, Train Acc: 4.66%\n",
      "Val Loss: 4.5944, Val Acc: 1.02%, Val Top5: 5.98%, Val Top10: 12.30%\n",
      "Learning Rate: 0.000120\n",
      "\n",
      "Epoch 23/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [02:46<00:00,  1.06it/s, Loss=4.239, Acc=5.20%]\n",
      "Validation: 100%|██████████| 20/20 [00:06<00:00,  3.28it/s, Loss=0.018, Top1=1.08%, Top5=6.10%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.2389, Train Acc: 5.20%\n",
      "Val Loss: 4.5921, Val Acc: 1.08%, Val Top5: 6.10%, Val Top10: 12.22%\n",
      "Learning Rate: 0.000120\n",
      "\n",
      "Epoch 24/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [02:46<00:00,  1.06it/s, Loss=4.214, Acc=5.54%]\n",
      "Validation: 100%|██████████| 20/20 [00:06<00:00,  3.30it/s, Loss=0.018, Top1=1.18%, Top5=6.32%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.2141, Train Acc: 5.54%\n",
      "Val Loss: 4.5891, Val Acc: 1.18%, Val Top5: 6.32%, Val Top10: 12.14%\n",
      "Learning Rate: 0.000120\n",
      "\n",
      "Epoch 25/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [02:46<00:00,  1.06it/s, Loss=4.212, Acc=5.86%]\n",
      "Validation: 100%|██████████| 20/20 [00:06<00:00,  3.30it/s, Loss=0.018, Top1=1.46%, Top5=6.56%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.2116, Train Acc: 5.86%\n",
      "Val Loss: 4.5875, Val Acc: 1.46%, Val Top5: 6.56%, Val Top10: 12.84%\n",
      "Learning Rate: 0.000120\n",
      "✓ New best validation accuracy: 1.46%\n",
      "\n",
      "Epoch 26/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [02:46<00:00,  1.05it/s, Loss=4.159, Acc=6.02%]\n",
      "Validation: 100%|██████████| 20/20 [00:06<00:00,  3.29it/s, Loss=0.018, Top1=1.44%, Top5=6.40%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.1595, Train Acc: 6.02%\n",
      "Val Loss: 4.5858, Val Acc: 1.44%, Val Top5: 6.40%, Val Top10: 13.16%\n",
      "Learning Rate: 0.000120\n",
      "\n",
      "Epoch 27/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  63%|██████▎   | 111/176 [01:45<01:01,  1.06it/s, Loss=4.186, Acc=6.19%]"
     ]
    }
   ],
   "source": [
    "# Create data loaders with optimized settings\n",
    "# Split training data for validation\n",
    "train_size = int(0.9 * len(train_dataset))  # Use more training data\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_subset, val_subset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Optimized data loaders\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True,\n",
    "                         num_workers=4, pin_memory=True, persistent_workers=True,\n",
    "                         prefetch_factor=2)\n",
    "val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False,\n",
    "                        num_workers=4, pin_memory=True, persistent_workers=True,\n",
    "                        prefetch_factor=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
    "                         num_workers=4, pin_memory=True, persistent_workers=True,\n",
    "                         prefetch_factor=2)\n",
    "\n",
    "# Create model with Stochastic Depth\n",
    "model = CNNClassifier(num_classes=num_classes, mlp_head_units=mlp_head_units,\n",
    "                     drop_path_rate=stochastic_depth_rate).to(device)\n",
    "\n",
    "# Print model summary\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CNN MODEL FOR CIFAR-100\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  • Total trainable parameters: {count_parameters(model):,}\")\n",
    "print(f\"  • Batch size: {batch_size} (effective: {batch_size * accumulation_steps} with gradient accumulation)\")\n",
    "print(f\"  • Training epochs: {num_epochs}\")\n",
    "print(f\"  • Initial learning rate: {initial_learning_rate}\")\n",
    "print(f\"  • Weight decay: {weight_decay}\")\n",
    "print(f\"  • Gradient accumulation steps: {accumulation_steps}\")\n",
    "print(f\"\\nFast Training Optimizations:\")\n",
    "print(f\"  ⚡ OneCycleLR scheduler (superconvergence)\")\n",
    "print(f\"  ⚡ Mixed precision training (FP16)\")\n",
    "print(f\"  ⚡ Gradient accumulation (2x effective batch)\")\n",
    "print(f\"  ⚡ Larger batch size for stability\")\n",
    "print(f\"  ⚡ Higher learning rate (0.003)\")\n",
    "print(f\"  ⚡ Aggressive augmentation (mixup_prob={mixup_prob})\")\n",
    "print(f\"\\nRegularization:\")\n",
    "print(f\"  • Stochastic depth: {stochastic_depth_rate}\")\n",
    "print(f\"  • EMA decay: {ema_decay}\")\n",
    "print(f\"  • Label smoothing: {label_smoothing}\")\n",
    "print(f\"  • Mixup alpha: {mixup_alpha}\")\n",
    "print(f\"  • CutMix alpha: {cutmix_alpha}\")\n",
    "print(f\"\\nAugmentation Strategy:\")\n",
    "print(f\"  • AutoAugment (CIFAR10 policy)\")\n",
    "print(f\"  • RandAugment (2 ops, magnitude 9)\")\n",
    "print(f\"  • Random Erasing (p=0.25)\")\n",
    "print(f\"  • Mixup/CutMix (p={mixup_prob})\")\n",
    "print(f\"\\nArchitecture Features:\")\n",
    "print(f\"  • FusedMBConv blocks (EfficientNetV2)\")\n",
    "print(f\"  • Stochastic Depth regularization\")\n",
    "print(f\"  • Squeeze-and-Excitation attention\")\n",
    "print(f\"  • Multi-scale global pooling\")\n",
    "print(f\"  • Exponential Moving Average\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Train the model\n",
    "print(\"Expected completion: ~50 epochs with early stopping\")\n",
    "model, history = train_model(model, train_loader, val_loader, num_epochs, device)\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL EVALUATION ON TEST SET\")\n",
    "print(\"=\" * 70)\n",
    "criterion = LabelSmoothingCrossEntropy(smoothing=0.0)  # No smoothing for evaluation\n",
    "test_loss, test_acc, test_top5_acc, test_top10_acc = validate(model, test_loader, criterion, device)\n",
    "\n",
    "print(f\"\\n📊 Final Test Results:\")\n",
    "print(f\"  • Test accuracy (Top-1): {test_acc:.2f}%\")\n",
    "print(f\"  • Test top-5 accuracy:   {test_top5_acc:.2f}%\")\n",
    "print(f\"  • Test top-10 accuracy:  {test_top10_acc:.2f}%\")\n",
    "print(f\"  • Test loss:             {test_loss:.4f}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Save final model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'test_acc': test_acc,\n",
    "    'test_top5_acc': test_top5_acc,\n",
    "    'test_top10_acc': test_top10_acc,\n",
    "    'history': history\n",
    "}, 'final_sota_model.pth')\n",
    "print(\"\\n✓ Model saved as 'final_sota_model.pth'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b398bf",
   "metadata": {
    "id": "f7b398bf"
   },
   "source": [
    "## Plot Training and Testing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4598e65b",
   "metadata": {
    "id": "4598e65b"
   },
   "outputs": [],
   "source": [
    "def plot_training_and_testing_results(history, test_acc, test_top5_acc, test_top10_acc, test_loss):\n",
    "    \"\"\"Plot comprehensive training, validation, and testing results\"\"\"\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "    # 1. Training and Validation Loss\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    ax1.plot(epochs, history['train_loss'], 'b-o', label='Training Loss', linewidth=2, markersize=4)\n",
    "    ax1.plot(epochs, history['val_loss'], 'r-s', label='Validation Loss', linewidth=2, markersize=4)\n",
    "    ax1.set_title('Training vs Validation Loss', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Loss', fontsize=12)\n",
    "    ax1.legend(fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. Training and Validation Accuracy\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    ax2.plot(epochs, history['train_acc'], 'b-o', label='Training Accuracy', linewidth=2, markersize=4)\n",
    "    ax2.plot(epochs, history['val_acc'], 'r-s', label='Validation Accuracy', linewidth=2, markersize=4)\n",
    "    ax2.set_title('Training vs Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. Top-K Validation Accuracies\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    ax3.plot(epochs, history['val_acc'], 'g-o', label='Top-1', linewidth=2, markersize=4)\n",
    "    ax3.plot(epochs, history['val_top5_acc'], 'orange', marker='s', label='Top-5', linewidth=2, markersize=4)\n",
    "    ax3.plot(epochs, history['val_top10_acc'], 'purple', marker='^', label='Top-10', linewidth=2, markersize=4)\n",
    "    ax3.set_title('Validation Top-K Accuracy', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xlabel('Epoch', fontsize=12)\n",
    "    ax3.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "    ax3.legend(fontsize=10)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "    # 4. Learning Rate Schedule\n",
    "    ax4 = fig.add_subplot(gs[1, 0])\n",
    "    ax4.plot(epochs, history['lr'], 'c-', linewidth=2)\n",
    "    ax4.set_title('Learning Rate Schedule (OneCycleLR)', fontsize=14, fontweight='bold')\n",
    "    ax4.set_xlabel('Epoch', fontsize=12)\n",
    "    ax4.set_ylabel('Learning Rate', fontsize=12)\n",
    "    ax4.set_yscale('log')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "    # 5. Final Performance Comparison - Bar Chart\n",
    "    ax5 = fig.add_subplot(gs[1, 1])\n",
    "    final_train_acc = history['train_acc'][-1]\n",
    "    final_val_acc = history['val_acc'][-1]\n",
    "\n",
    "    categories = ['Training', 'Validation', 'Test']\n",
    "    accuracies = [final_train_acc, final_val_acc, test_acc]\n",
    "    colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "\n",
    "    bars = ax5.bar(categories, accuracies, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "    ax5.set_title('Final Top-1 Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "    ax5.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "    ax5.set_ylim([0, 100])\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        height = bar.get_height()\n",
    "        ax5.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{acc:.2f}%',\n",
    "                ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "    ax5.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # 6. Top-K Test Accuracy Comparison\n",
    "    ax6 = fig.add_subplot(gs[1, 2])\n",
    "    topk_categories = ['Top-1', 'Top-5', 'Top-10']\n",
    "    topk_accuracies = [test_acc, test_top5_acc, test_top10_acc]\n",
    "    topk_colors = ['#e74c3c', '#f39c12', '#9b59b6']\n",
    "\n",
    "    bars2 = ax6.bar(topk_categories, topk_accuracies, color=topk_colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "    ax6.set_title('Test Set Top-K Accuracy', fontsize=14, fontweight='bold')\n",
    "    ax6.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "    ax6.set_ylim([0, 100])\n",
    "\n",
    "    # Add value labels\n",
    "    for bar, acc in zip(bars2, topk_accuracies):\n",
    "        height = bar.get_height()\n",
    "        ax6.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{acc:.2f}%',\n",
    "                ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "    ax6.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # 7. Accuracy Progression (All Sets)\n",
    "    ax7 = fig.add_subplot(gs[2, :2])\n",
    "    ax7.plot(epochs, history['train_acc'], 'b-o', label='Training', linewidth=2.5, markersize=5, alpha=0.8)\n",
    "    ax7.plot(epochs, history['val_acc'], 'r-s', label='Validation', linewidth=2.5, markersize=5, alpha=0.8)\n",
    "    ax7.axhline(y=test_acc, color='g', linestyle='--', linewidth=3, label=f'Test (Final: {test_acc:.2f}%)')\n",
    "    ax7.fill_between(epochs, history['train_acc'], alpha=0.2, color='blue')\n",
    "    ax7.fill_between(epochs, history['val_acc'], alpha=0.2, color='red')\n",
    "    ax7.set_title('Accuracy Progression: Train vs Val vs Test', fontsize=14, fontweight='bold')\n",
    "    ax7.set_xlabel('Epoch', fontsize=12)\n",
    "    ax7.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "    ax7.legend(fontsize=11, loc='lower right')\n",
    "    ax7.grid(True, alpha=0.3)\n",
    "\n",
    "    # 8. Performance Summary Table\n",
    "    ax8 = fig.add_subplot(gs[2, 2])\n",
    "    ax8.axis('off')\n",
    "\n",
    "    summary_data = [\n",
    "        ['Metric', 'Train', 'Val', 'Test'],\n",
    "        ['Top-1 Acc', f'{final_train_acc:.2f}%', f'{final_val_acc:.2f}%', f'{test_acc:.2f}%'],\n",
    "        ['Top-5 Acc', '-', f'{history[\"val_top5_acc\"][-1]:.2f}%', f'{test_top5_acc:.2f}%'],\n",
    "        ['Top-10 Acc', '-', f'{history[\"val_top10_acc\"][-1]:.2f}%', f'{test_top10_acc:.2f}%'],\n",
    "        ['Loss', f'{history[\"train_loss\"][-1]:.4f}', f'{history[\"val_loss\"][-1]:.4f}', f'{test_loss:.4f}'],\n",
    "        ['Epochs', str(len(epochs)), '-', '-']\n",
    "    ]\n",
    "\n",
    "    table = ax8.table(cellText=summary_data, cellLoc='center', loc='center',\n",
    "                     colWidths=[0.25, 0.25, 0.25, 0.25])\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1, 2.5)\n",
    "\n",
    "    # Style the header row\n",
    "    for i in range(4):\n",
    "        table[(0, i)].set_facecolor('#34495e')\n",
    "        table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "    # Alternate row colors\n",
    "    for i in range(1, len(summary_data)):\n",
    "        for j in range(4):\n",
    "            if i % 2 == 0:\n",
    "                table[(i, j)].set_facecolor('#ecf0f1')\n",
    "            else:\n",
    "                table[(i, j)].set_facecolor('#ffffff')\n",
    "\n",
    "    ax8.set_title('Performance Summary', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "    plt.suptitle('Complete Training and Testing Analysis', fontsize=18, fontweight='bold', y=0.995)\n",
    "    plt.savefig('training_testing_results.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"\\n✓ Plot saved as 'training_testing_results.png'\")\n",
    "    plt.show()\n",
    "\n",
    "# Plot comprehensive results\n",
    "plot_training_and_testing_results(history, test_acc, test_top5_acc, test_top10_acc, test_loss)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
