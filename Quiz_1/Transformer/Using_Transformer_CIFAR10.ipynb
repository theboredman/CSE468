{"cells":[{"cell_type":"markdown","metadata":{"id":"jDDefb7wOwvp"},"source":["# Vision Transformer (CIFAR 10 Dataset)"]},{"cell_type":"markdown","metadata":{"id":"BznMYM4BOwvq"},"source":["## Setup"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1760468765224,"user":{"displayName":"Asadullah Hil Galib","userId":"02825503865158832131"},"user_tz":-360},"id":"I5dp3f2tOwvq"},"outputs":[],"source":["import os\n","\n","os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  # @param [\"tensorflow\", \"jax\", \"torch\"]\n","\n","import keras\n","from keras import layers\n","from keras import ops\n","\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"6ZKoW6mNOwvq"},"source":["## Prepare the data"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2780,"status":"ok","timestamp":1760468768021,"user":{"displayName":"Asadullah Hil Galib","userId":"02825503865158832131"},"user_tz":-360},"id":"QyJ7EmyOOwvr","outputId":"a6c61bac-e944-4b6f-f73d-50a344d30ae6"},"outputs":[{"output_type":"stream","name":"stdout","text":["x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n","x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n","Data range after normalization: [0.000, 1.000]\n"]}],"source":["num_classes = 10\n","input_shape = (32, 32, 3)\n","\n","(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n","\n","print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n","print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n","\n","# Convert to float32 and normalize pixel values to [0, 1]\n","x_train = x_train.astype('float32') / 255.0\n","x_test = x_test.astype('float32') / 255.0\n","\n","print(f\"Data range after normalization: [{x_train.min():.3f}, {x_train.max():.3f}]\")"]},{"cell_type":"markdown","metadata":{"id":"D09NY6oNOwvr"},"source":["## Configure the hyperparameters"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1760468768043,"user":{"displayName":"Asadullah Hil Galib","userId":"02825503865158832131"},"user_tz":-360},"id":"uUj-whd4Owvr"},"outputs":[],"source":["learning_rate = 0.0005  # Slightly reduced for better convergence\n","weight_decay = 0.0001\n","batch_size = 256\n","num_epochs = 10  # For real training, use num_epochs=100. 10 is a test value\n","image_size = 72  # We'll resize input images to this size\n","patch_size = 6  # Size of the patches to be extract from the input images\n","num_patches = (image_size // patch_size) ** 2\n","projection_dim = 128  # Increased from 64 for better representation\n","num_heads = 8  # Increased from 4 for more attention diversity\n","transformer_units = [\n","    projection_dim * 4,  # Increased FFN expansion ratio to 4x\n","    projection_dim,\n","]  # Size of the transformer layers\n","transformer_layers = 12  # Increased from 8 for deeper architecture\n","mlp_head_units = [\n","    1024,\n","    512,\n","]  # Size of the dense layers of the final classifier\n","dropout_rate = 0.1\n","stochastic_depth_rate = 0.1  # For stochastic depth regularization\n"]},{"cell_type":"markdown","metadata":{"id":"TCG6llz9Owvr"},"source":["## Use data augmentation"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1760468768058,"user":{"displayName":"Asadullah Hil Galib","userId":"02825503865158832131"},"user_tz":-360},"id":"EqkQlgZYOwvs"},"outputs":[],"source":["data_augmentation = keras.Sequential(\n","    [\n","        layers.Resizing(image_size, image_size),\n","        layers.RandomFlip(\"horizontal\"),\n","        layers.RandomRotation(factor=0.02),\n","        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n","        layers.RandomTranslation(height_factor=0.1, width_factor=0.1),  # Added translation\n","        layers.RandomContrast(factor=0.2),  # Added contrast augmentation\n","    ],\n","    name=\"data_augmentation\",\n",")\n","# Note: Normalization is now done explicitly in the data preparation step\n"]},{"cell_type":"markdown","metadata":{"id":"vCbpyizwOwvs"},"source":["## Implement multilayer perceptron (MLP)"]},{"cell_type":"markdown","metadata":{"id":"l9xAtCK4HXQ5"},"source":["## Implement Stochastic Depth (DropPath) for regularization\n"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"CxoW30QhHXQ5","executionInfo":{"status":"ok","timestamp":1760468768073,"user_tz":-360,"elapsed":3,"user":{"displayName":"Asadullah Hil Galib","userId":"02825503865158832131"}}},"outputs":[],"source":["class StochasticDepth(layers.Layer):\n","    \"\"\"Stochastic Depth layer for regularization.\n","\n","    References:\n","    - https://arxiv.org/abs/1603.09382\n","    \"\"\"\n","    def __init__(self, drop_prob=0.0, **kwargs):\n","        super().__init__(**kwargs)\n","        self.drop_prob = drop_prob\n","\n","    def call(self, x, training=None):\n","        if training:\n","            keep_prob = 1 - self.drop_prob\n","            shape = (ops.shape(x)[0],) + (1,) * (len(x.shape) - 1)\n","            random_tensor = keep_prob + keras.random.uniform(shape, 0, 1)\n","            random_tensor = ops.floor(random_tensor)\n","            return (x / keep_prob) * random_tensor\n","        return x\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\"drop_prob\": self.drop_prob})\n","        return config\n"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1760468768078,"user":{"displayName":"Asadullah Hil Galib","userId":"02825503865158832131"},"user_tz":-360},"id":"t9vc7ZScOwvs"},"outputs":[],"source":["\n","def mlp(x, hidden_units, dropout_rate):\n","    for units in hidden_units:\n","        x = layers.Dense(units, activation=keras.activations.gelu)(x)\n","        x = layers.Dropout(dropout_rate)(x)\n","    return x\n"]},{"cell_type":"markdown","metadata":{"id":"qY1MRkfDOwvs"},"source":["## Implement patch creation as a layer"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":24,"status":"ok","timestamp":1760468768104,"user":{"displayName":"Asadullah Hil Galib","userId":"02825503865158832131"},"user_tz":-360},"id":"1FyoVcvNOwvs"},"outputs":[],"source":["class Patches(layers.Layer):\n","    def __init__(self, patch_size):\n","        super().__init__()\n","        self.patch_size = patch_size\n","\n","    def call(self, images):\n","        # Images are already float32 from preprocessing\n","        input_shape = ops.shape(images)\n","        batch_size = input_shape[0]\n","        height = input_shape[1]\n","        width = input_shape[2]\n","        channels = input_shape[3]\n","        num_patches_h = height // self.patch_size\n","        num_patches_w = width // self.patch_size\n","        patches = keras.ops.image.extract_patches(images, size=self.patch_size)\n","        patches = ops.reshape(\n","            patches,\n","            (\n","                batch_size,\n","                num_patches_h * num_patches_w,\n","                self.patch_size * self.patch_size * channels,\n","            ),\n","        )\n","        return patches\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\"patch_size\": self.patch_size})\n","        return config"]},{"cell_type":"markdown","metadata":{"id":"wUzk2qH0Owvs"},"source":["Let's display patches for a sample image"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":746},"executionInfo":{"elapsed":2169,"status":"ok","timestamp":1760468770275,"user":{"displayName":"Asadullah Hil Galib","userId":"02825503865158832131"},"user_tz":-360},"id":"I1iDtJoQOwvs","outputId":"d6cda4e5-ccb4-436b-afce-b830a7f72717"},"outputs":[{"output_type":"stream","name":"stdout","text":["Image size: 72 X 72\n","Patch size: 6 X 6\n","Patches per image: 144\n","Elements per patch: 108\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 400x400 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAUgAAAFICAYAAAAyFGczAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAABDhJREFUeJzt27ENhEAMAEEO0X/LpgC00iX8E8zEDhytnHjNzBwAPJz/XgDgqwQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRCu3cG11pt7APzM7gOhCxIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIBw7Q7OzJt7AHyOCxIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSINwAOgyN2LrQOQAAAABJRU5ErkJggg==\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 400x400 with 144 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["plt.figure(figsize=(4, 4))\n","image = x_train[np.random.choice(range(x_train.shape[0]))]\n","plt.imshow(image.astype(\"uint8\"))\n","plt.axis(\"off\")\n","\n","resized_image = ops.image.resize(\n","    ops.convert_to_tensor([image]), size=(image_size, image_size)\n",")\n","patches = Patches(patch_size)(resized_image)\n","print(f\"Image size: {image_size} X {image_size}\")\n","print(f\"Patch size: {patch_size} X {patch_size}\")\n","print(f\"Patches per image: {patches.shape[1]}\")\n","print(f\"Elements per patch: {patches.shape[-1]}\")\n","\n","n = int(np.sqrt(patches.shape[1]))\n","plt.figure(figsize=(4, 4))\n","for i, patch in enumerate(patches[0]):\n","    ax = plt.subplot(n, n, i + 1)\n","    patch_img = ops.reshape(patch, (patch_size, patch_size, 3))\n","    plt.imshow(ops.convert_to_numpy(patch_img).astype(\"uint8\"))\n","    plt.axis(\"off\")"]},{"cell_type":"markdown","metadata":{"id":"qeOuAlddOwvt"},"source":["## Implement the patch encoding layer\n","\n","The `PatchEncoder` layer will linearly transform a patch by projecting it into a\n","vector of size `projection_dim`. In addition, it adds a learnable position\n","embedding to the projected vector."]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1760468770288,"user":{"displayName":"Asadullah Hil Galib","userId":"02825503865158832131"},"user_tz":-360},"id":"-OWv22BBOwvt"},"outputs":[],"source":["\n","class PatchEncoder(layers.Layer):\n","    def __init__(self, num_patches, projection_dim):\n","        super().__init__()\n","        self.num_patches = num_patches\n","        self.projection = layers.Dense(units=projection_dim)\n","        self.position_embedding = layers.Embedding(\n","            input_dim=num_patches + 1, output_dim=projection_dim  # +1 for CLS token\n","        )\n","\n","    def build(self, input_shape):\n","        # Create a learnable CLS token\n","        self.cls_token = self.add_weight(\n","            shape=(1, 1, input_shape[-1]),\n","            initializer=\"zeros\",\n","            trainable=True,\n","            name=\"cls_token\"\n","        )\n","        super().build(input_shape)\n","\n","    def call(self, patch):\n","        batch_size = ops.shape(patch)[0]\n","        # Expand CLS token for the batch\n","        cls_token = ops.broadcast_to(\n","            self.cls_token, (batch_size, 1, ops.shape(patch)[-1])\n","        )\n","        # Concatenate CLS token with patches\n","        patch = ops.concatenate([cls_token, patch], axis=1)\n","\n","        # Create position indices including CLS token\n","        positions = ops.expand_dims(\n","            ops.arange(start=0, stop=self.num_patches + 1, step=1), axis=0\n","        )\n","        projected_patches = self.projection(patch)\n","        encoded = projected_patches + self.position_embedding(positions)\n","        return encoded\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\"num_patches\": self.num_patches, \"projection_dim\": self.projection_dim})\n","        return config\n"]},{"cell_type":"markdown","metadata":{"id":"bb0c9v13Owvt"},"source":["## Build the Improved ViT model\n","\n","The improved ViT model includes several enhancements over the baseline:\n","\n","**Architecture Improvements:**\n","1. **CLS Token**: Added a learnable classification token (similar to BERT) that aggregates information from all patches through self-attention\n","2. **Stochastic Depth**: Implements DropPath regularization with linearly increasing drop probability across layers for better training\n","3. **Increased Capacity**:\n","   - Projection dimension: 64 → 128\n","   - Number of heads: 4 → 8\n","   - Transformer layers: 8 → 12\n","   - FFN expansion ratio: 2x → 4x\n","4. **Better Pooling**: Uses CLS token instead of flattening all patches, reducing parameters and improving representation\n","\n","**Training Improvements:**\n","1. **Enhanced Data Augmentation**: Added RandomTranslation and RandomContrast\n","2. **Optimized Learning Rate**: Reduced to 0.0005 for more stable convergence\n","3. **Regularization**: Stochastic depth with 0.1 drop rate prevents overfitting\n","\n","These improvements are based on modern ViT architectures like DeiT and recent research findings.\n"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1760468770309,"user":{"displayName":"Asadullah Hil Galib","userId":"02825503865158832131"},"user_tz":-360},"id":"r0eACAmmOwvt"},"outputs":[],"source":["\n","def create_vit_classifier():\n","    inputs = keras.Input(shape=input_shape)\n","    # Augment data.\n","    augmented = data_augmentation(inputs)\n","    # Create patches.\n","    patches = Patches(patch_size)(augmented)\n","    # Encode patches (includes CLS token).\n","    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n","\n","    # Stochastic depth schedule - linearly increasing drop probability\n","    dpr = [x for x in ops.linspace(0, stochastic_depth_rate, transformer_layers)]\n","\n","    # Create multiple layers of the Transformer block.\n","    for i in range(transformer_layers):\n","        # Layer normalization 1.\n","        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n","        # Create a multi-head attention layer.\n","        attention_output = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=projection_dim // num_heads, dropout=dropout_rate\n","        )(x1, x1)\n","        # Apply stochastic depth\n","        attention_output = StochasticDepth(dpr[i])(attention_output)\n","        # Skip connection 1.\n","        x2 = layers.Add()([attention_output, encoded_patches])\n","        # Layer normalization 2.\n","        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n","        # MLP.\n","        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=dropout_rate)\n","        # Apply stochastic depth\n","        x3 = StochasticDepth(dpr[i])(x3)\n","        # Skip connection 2.\n","        encoded_patches = layers.Add()([x3, x2])\n","\n","    # Final layer normalization\n","    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n","\n","    # Extract CLS token for classification (first token)\n","    cls_token = representation[:, 0]\n","\n","    # Add MLP classification head\n","    features = mlp(cls_token, hidden_units=mlp_head_units, dropout_rate=0.3)\n","\n","    # Classify outputs.\n","    logits = layers.Dense(num_classes)(features)\n","    # Create the Keras model.\n","    model = keras.Model(inputs=inputs, outputs=logits)\n","    return model\n"]},{"cell_type":"markdown","metadata":{"id":"E5AfzEtOOwvt"},"source":["## Compile, train, and evaluate the mode"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jeN5G9j-Owvt","outputId":"7c54c142-ec9c-48e6-930c-9384a07a6e9b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 739ms/step - accuracy: 0.1825 - loss: 2.1434 - top-5-accuracy: 0.6769 - val_accuracy: 0.3852 - val_loss: 1.6584 - val_top-5-accuracy: 0.8848\n","Epoch 2/10\n","\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 727ms/step - accuracy: 0.3526 - loss: 1.7175 - top-5-accuracy: 0.8769 - val_accuracy: 0.4640 - val_loss: 1.4388 - val_top-5-accuracy: 0.9288\n","Epoch 3/10\n","\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 727ms/step - accuracy: 0.4419 - loss: 1.5160 - top-5-accuracy: 0.9144 - val_accuracy: 0.5018 - val_loss: 1.3619 - val_top-5-accuracy: 0.9304\n","Epoch 4/10\n","\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 727ms/step - accuracy: 0.4979 - loss: 1.3833 - top-5-accuracy: 0.9302 - val_accuracy: 0.5484 - val_loss: 1.2201 - val_top-5-accuracy: 0.9566\n","Epoch 5/10\n","\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 727ms/step - accuracy: 0.5222 - loss: 1.3074 - top-5-accuracy: 0.9422 - val_accuracy: 0.5718 - val_loss: 1.1558 - val_top-5-accuracy: 0.9550\n","Epoch 6/10\n","\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 727ms/step - accuracy: 0.5542 - loss: 1.2418 - top-5-accuracy: 0.9462 - val_accuracy: 0.5982 - val_loss: 1.1086 - val_top-5-accuracy: 0.9614\n","Epoch 7/10\n","\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 726ms/step - accuracy: 0.5607 - loss: 1.2129 - top-5-accuracy: 0.9490 - val_accuracy: 0.6038 - val_loss: 1.1178 - val_top-5-accuracy: 0.9600\n","Epoch 8/10\n","\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 727ms/step - accuracy: 0.5811 - loss: 1.1630 - top-5-accuracy: 0.9559 - val_accuracy: 0.6184 - val_loss: 1.0580 - val_top-5-accuracy: 0.9614\n","Epoch 9/10\n","\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 723ms/step - accuracy: 0.5920 - loss: 1.1327 - top-5-accuracy: 0.9591 - val_accuracy: 0.6070 - val_loss: 1.0877 - val_top-5-accuracy: 0.9610\n","Epoch 10/10\n","\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702ms/step - accuracy: 0.6075 - loss: 1.0967 - top-5-accuracy: 0.9617"]}],"source":["\n","def run_experiment(model):\n","    optimizer = keras.optimizers.AdamW(\n","        learning_rate=learning_rate, weight_decay=weight_decay\n","    )\n","\n","    model.compile(\n","        optimizer=optimizer,\n","        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","        metrics=[\n","            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n","            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n","        ],\n","    )\n","\n","    checkpoint_filepath = \"/tmp/checkpoint.weights.h5\"\n","    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n","        checkpoint_filepath,\n","        monitor=\"val_accuracy\",\n","        save_best_only=True,\n","        save_weights_only=True,\n","    )\n","\n","    history = model.fit(\n","        x=x_train,\n","        y=y_train,\n","        batch_size=batch_size,\n","        epochs=num_epochs,\n","        validation_split=0.1,\n","        callbacks=[checkpoint_callback],\n","    )\n","\n","    model.load_weights(checkpoint_filepath)\n","    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n","    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n","    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n","\n","    return history\n","\n","\n","vit_classifier = create_vit_classifier()\n","history = run_experiment(vit_classifier)\n","\n","\n","def plot_history(item):\n","    plt.plot(history.history[item], label=item)\n","    plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n","    plt.xlabel(\"Epochs\")\n","    plt.ylabel(item)\n","    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n","    plt.legend()\n","    plt.grid()\n","    plt.show()\n","\n","\n","plot_history(\"loss\")\n","plot_history(\"top-5-accuracy\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"nbformat":4,"nbformat_minor":0}